---
documentclass: jss
bibliography: article.bib
author:
  - name: FirstName LastName
    orcid: 0000-0000-0000-0000
    affiliation: University/Company
    # use this syntax to add text on several lines
    address: |
      | First line
      | Second line
    email: \email{name@company.com}
    url: https://posit.co
  - name: Second Author
    orcid: 0000-0000-0000-0000
    affiliation: 'Affiliation \AND'
    # To add another line, use \AND at the end of the previous one as above
  - name: Third Author
    orcid: 0000-0000-0000-0000
    affiliation: 'Affiliation'
    # To add another line, use \AND at the end of the previous one as above
  - name: Third Author
    orcid: 0000-0000-0000-0000
    affiliation: 'Affiliation \AND'
    # To add another line, use \AND at the end of the previous one as above
  - name: Third Author
    orcid: 0000-0000-0000-0000
    affiliation: 'Affiliation'
    # To add another line, use \AND at the end of the previous one as above
  - name: Fourth Author
    orcid: 0000-0000-0000-0000
    address: |
      | Department of Statistics and Mathematics,
      | Faculty of Biosciences,
      | Universitat Autònoma de Barcelona
    affiliation: |
      | Universitat Autònoma 
      | de Barcelona
    # use a different affiliation in adress field (differently formated here)
    affiliation2: Universitat Autònoma de Barcelona
title:
  formatted: "Reservoir computing in R : a tutorial using \\pkg{reservoirnet}"
  # If you use tex in the formatted title, also supply version without
  plain:     "Reservoir computing in R : a tutorial using reservoirnet"
  # For running headers, if needed
  short:     "\\pkg{reservoirnet}: reservoir computing in R"
abstract: >
  The abstract of the article.
keywords:
  # at least one keyword must be supplied
  formatted: [keywords, not capitalized, "\\proglang{Java}"]
  plain:     [keywords, not capitalized, Java]
preamble: >
  \usepackage{amsmath}
output:
  bookdown::pdf_book:
    base_format: rticles::jss_article
  bookdown::word_document2:
    default
knit: (function(inputFile, encoding){
  rmarkdown::render(inputFile, encoding = encoding,
  output_dir = here::here("draft_jss"), output_format = "all") })
---

```{r, setup, include=FALSE}
options(prompt = 'R> ', continue = '+ ')
set.seed(1)
```

# Introduction

## Reservoir networks presentation

Reservoir Computing (RC) is a prominent machine learning methodology
that has gained significant attention in recent years for its ability to
effectively process information generated by dynamical systems. This
innovative approach leverages the dynamics of a high-dimensional
reservoir to perform complex computations and solve various tasks based
on the response of this dynamical system to input signals. Reservoir
Computing is characterized by its unique architecture, which includes a
dynamic system, often referred to as a reservoir, that projects temporal
input signals onto a high-dimensional feature space. This
high-dimensional representation of input data enables Reservoir
Computing to excel in tasks such as time series prediction, pattern
recognition, and signal processing.

## Why reservoir are interesting compared to standard approaches (penalised regression) and deep learning approaches?

RC exhibits several advantages over its counterparts. It excels in tasks
that require the processing of temporal data, such as time series
forecasting and speech recognition, thanks to its inherent memory
capabilities. RC offers computational efficiency, reduced susceptibility
to overfitting, and a higher degree of interpretability compared to deep
learning models. Moreover, RC is able to capture intricate temporal
patterns compared to statistical methods such as penalized regression.

## examples of previous use

TODO: - Ghosh S, Senapati A, Mishra A, Chattopadhyay J, Dana SK, Hens C,
et al. Reservoir computing on epidemic spreading: A case study on
COVID-19 cases. Phys Rev E. 16 juill 2021;104(1):014308. - Existing
review?

## challenges of using reservoir

RC use is stille challenging for several reasons. First, its stochastic
nature due to the reservoir random part makes the results of a given RC
unpredictable even if the hyperparameters are well chosen. Second,
eventhough, RC relies on several hyperparameter to determine the
connections between neurons. There exist few guidance about the way to
set the hyperparameters and most often the user must rely on a wrapper
approach were combinations of hyperparameters performance are evaluated
on a train set before chosing the ideal combination one. Third, most
published paper are based on a low dimensional setting and few guidance
exist in the context of high dimensional data, especially considering
the choice of hyperparameters. Fourth, the use of reservoir computing in
an epidemic setting with highly non-stationary time serie is scarce
whereas it is of tremendous interest for epidemiologists. Fifth, there
is currently no implementation available on R making the use of this
method challenging for user not familiar with Python.

## what we do

In this paper we propose to explore those challenges with extensive
guidance in order to help new user to fully benefit from RC. First a
general introduction to reservoir computing is provided with a tutorial
for its use using reservoirnet, a R package based on reservoirPy Python
module. Second, we explore the different challenges of the use of RC in
a non-stationary high dimensional setting based on the use case of
covid-19 hospitalisations forecast with extensive guidance on the
modelling strategy, the choice of hyperparameters using a genetic
algorithm and the implementation.

# RC presentation

RC is a machine learning algorithm composed of an input layer denoted
$u(t)$ which is randomy transformed by the reservoir into an activation
state denoted $x(t)$ which is fed to a ridge penalized linear regression
trained to foreast the outcome denoted as $y(t)$ as depicted at figure
\@ref(fig:rcpresentation).

```{r rcpresentation, echo=FALSE, fig.cap="my caption", out.width = '90%', fig.cap="Reservoir computing is composed of an input layer, a reservoir and an output layer. Connection between input layer and reservoir and inside reservoir are random. Only the output layer is optimized based on a ridge penalized linear regression."}
knitr::include_graphics("images/schema.png/image1.png")
```

The input layer $u(t)$ is an $M$-dimension vector which corresponds to
the values of the input time series at time $t$ where $t = 1, …, T$. The
reservoir layer $x(t)$ is an $N_{res}$-dimensional vector where
$N_{res}$ is the number of nodes in the reservoir. The value $x(t)$ is
defined as follow:

$$x( t+1 ) = ( 1 - \alpha )  x ( t) + \alpha \: tanh( W x(t) + W_{in} u(t+1) ) \text{ where } \alpha \in [0, 1 ]$$

The leaking rate alpha define the rate of update of the nodes. The
closer $\alpha$ is to $1$, the higher the reservoir is sensitive to new
inputs (i.e $u(t)$). Therefore, the reservoir state at time $t+1$
denoted $x(t+1)$ depends on the reservoir state at the previous time
(i.e $x(t)$) and the new inputs (i.e $u(t+1)$). Both $W_{in}$ and $W$
are random matrices of size $Nres \times M$ and $Nres \times Nres$
respectively.

$W_{in}$ is a dense matrix generated using a bernouilli distribution
where each value can be either $-I_{scale}(m)$ or $I_{scale}(m)$ with an
equal probability where $m = 1, …, M$ corresponds to a given feature in
the input layer. The input scaling, denoted $I_{scale}$, is an
hyperparameter coefficient which can be common to all features from the
input layer or specific to each feature $m$. In that case, the more
important the feature is, the greater should be its input scaling. $W$
is a sparse matrix where values are generated base on a gaussian
distribution $\mathcal{N}(0,1)$. Then the matrix $W$ is rescaled
according to the spectral radius, an hyperparameter defining the highest
eigen value of $W$.

The final layer is a linear regression with ridge penalization where the
explanatory features are the reservoir state and the variable to be
explain is the outcome to predict such that:

$$W_{out} = YX^T ( XX^T + \lambda  I)^{ -1 }$$

Where x(t) and y(t) are accumulated in X and Y respectively such that:

$$X = \begin{bmatrix} x(1) \\ x(2) \\ ... \\ x(T) \end{bmatrix}
\text{ and } Y = \begin{bmatrix} y(1) \\ y(2) \\ ... \\ y(T) \end{bmatrix}$$

The parameter $\lambda$ is the ridge penalisation which aims to prevent
overfitting.

Overall, there are four main hyperparameters to be chosen by the user:
the leaking rate wich defines the memory of the RC, the input scaling
wich define the relative importance of the features, the spectral radius
wich define the connections of the neurons inside the reservoir and the
ridge penalization wich defines the degree of overfitting. The choice of
hyperparameter is difficult and the use of a wrapper approach where the
performance of the RC with different combinations of hyperparameters is
evaluated on a train set, the best combination is chosen for the use on
the test set.

RC in high dimension

The interest of RC in high dimensional setting is unclear for at least
two reasons: (i) RC relies primarily on the projection of the low
dimension input layer to the high dimension space of the reservoir
neurons and (ii) the high number of features for which each can be
associated with a different input scaling increases the hyperparameter
search space making standard approaches such as random search or grid
search inefficient.

To tackle both problems, we proposed a genetic algorithm for
hyperparameter optimisation including feature selection.

1.  Initialize :

-   Population parameters : $N_{pop} = 200$, $N_e = 100$,
    $N_{generation} = 30$
-   Cross-over parameters : $N_{tournament} = 2$
-   Mutation parameters : $p_{mutQuant} = .5$, $p_{mutCat} = .25$,
    $\sigma = 1$ ($\sigma = 0.1$ for leaking rate as the hyperparameter
    range is shorter)
-   Define the $hyperparameter\_list$, a list containing each
    hyperparameter denoted as $param$.
-   `for` each $param$ in $hyperparameter\_list$ define hyperparameter
    $search\_space$ :
    -   the $lower$ and $upper$ bounds,
    -   the $type$ (numeric, integer, categorical)
    -   if it should be $log$ transformed (True, False)

2.  Define main functions
    -   Define $tournamentSelection(N_{pop}, N_{tournament})$:
        -   Get the best $N_{pop}$ individuals from previous generations
        -   Randomly select $N_{tournament}$ challengers among them
        -   Get the best individuals from the challengers
        -   Repeat twice to get a father and a mother
        -   `return` $father, \: mother$
    -   Define
        $crossoverMutation(pere\$param\$value, mere\$param\$value, param\$search\_space)$:
        -   `if` $search\_space\$log$ $\newline$
            $\text{} \qquad pere\$param\$value = log_{10}(pere\$param\$value)$
            $\newline$
            $\text{} \qquad mere\$param\$value = log_{10}(mere\$param\$value)$

        -   sample $alp$ and $mut$ probabilities from $\mathcal{U}(0,1)$

        -   Cross-over, and define $child\$param\$value$ as :

            -   Numerical :
                $alp*pere\$param\$value + (1-alp)*mere\$param\$value$
            -   Integer :
                $round( alp*pere\$param\$value + (1-alp)*mere\$param\$value)$
            -   Categorical :
                $sample(x = c(pere\$param\$value, mere\$param\$value), \newline\text{}\qquad\text{}\qquad\text{}\qquad\text{}\qquad\text{}\qquad prob = c(alp, 1-alp))$

        -   `if` $mut < p_{mutQuant}$ or $mut < p_{mutCat}$ for
            quantitative and categorical features respectively, mutate :

            -   Categorical : sample another modality
            -   Integer : add or substract 1 at random
            -   Numerical : Add $\sigma \times \mathcal{N}(0,1)$

        -   `if` $search\_space\$log$ :
            $child\$param\$value = 10^{child\$param\$value}$

        -   `return` $child\$param\$value$
    -   Define $geneticSearch(N_{pop}, N_{tournament})$ :
        -   $pere, \: mere = tournamentSelection(N_{pop}, N_{tournament})$
        -   `for` $param$ in $hyperparameter\_list$ : $\newline$
            $\text{} \qquad child\$param\$value = crossoverMutation(\newline\text{}\qquad\text{}\qquad\text{}\qquad\text{}\qquad\text{}\qquad pere\$param\$value, \newline\text{}\qquad\text{}\qquad\text{}\qquad\text{}\qquad\text{}\qquad mere\$param\$value, \newline\text{}\qquad\text{}\qquad\text{}\qquad\text{}\qquad\text{}\qquad param\$search\_space)$
        -   `return` $child$
3.  Run the algorithm
    -   Step 1 : Initialize genetic algorithm
        -   Sample $N_{pop}$ individuals (i.e set of hyperparameters)
            using random search
    -   Step 2 : Run genetic algorithm
        -   $generation = 1$
        -   `while` $generation < Ngeneration$ :
            -   $generation = generation + 1$
            -   sample $N_{pop}$ individuals using $geneticSearch$
                function

# Basic package use

In this section, we will cover the basics of reservoirnet use including
installation, classification and regression. We will provide brief
overview of the package use and more in depth description is provided in
section 4 with the covid-19 forecast use case.

## Installation

reservoirnet is an R package api making the python module reservoirPy
easily callable from R. It is available on CRAN (see
<https://cran.r-project.org/package=reservoirnet>) and can be installed
using:

```{r eval=FALSE}
install.packages("reservoirnet")
```

Reservoir Computing (RC) is well suited to both regression and
classification tasks. We will introduce a simple example for both task.

## Classification

This example is largely inspired from the [classification tutorial of reservoirpy](https://github.com/reservoirpy/reservoirpy/blob/master/tutorials/5-Classification-with-RC.ipynb). The first step is to load useful packages. For this example we will load both reservoirnet, ggplot2 [@wickham_ggplot2_2016] and dplyr
[@wickham_dplyr_2023].

```{r message=FALSE, warning=FALSE}
library(reservoirnet)
library(ggplot2)
library(dplyr)
```

### The Japanese vowel dataset

To illustrate the classification task, we will use the Japanese vowel dataset (@kudo_multidimensional_1999). The data can be loaded from reservoirnet as follow :

```{r}
japanese_vowels <- reservoirnet::generate_data(dataset = "japanese_vowels")[[1]]
X_train <- japanese_vowels$X_train
Y_train <- japanese_vowels$Y_train
X_test <- japanese_vowels$X_test
Y_test <- japanese_vowels$Y_test
```

The dataset is composed of 640 utterances of the Japanese vowel \ae, from 9 different speakers. The goal is to assign each uterance to the its speaker. Each utterances is a time series of 7 to 29 timesteps encoded by a 12 dimensional vector representing Linear Prediction Coefficient (LPC).

```{r fig.cap="Vowel dataset, sample with 3 speakers and 2 utterance each."}
vec_sample <- c(1, 2, 41, 42, 71, 72)
dfplot_vowel <- lapply(vec_sample,
                 FUN = function(i){
                   speaker <- which(Y_test[[i]] == 1)
                   X_test[[i]] %>%
                     as.data.frame() %>%
                     tibble::rowid_to_column(var = "Time") %>%
                     tidyr::pivot_longer(cols = -Time,
                                         names_to = "component",
                                         values_to = "LPC") %>%
                     mutate(speaker = speaker, .before = 1,
                            uterrance = i) %>%
                     return()
                 }) %>%
  bind_rows()

ggplot(dfplot_vowel, mapping = aes(x = Time, y = LPC, color = component)) +
  geom_line() +
  facet_wrap(uterrance ~ speaker,
             labeller = label_bquote(cols = "speaker" : .(speaker)),
             ncol = 2) +
  theme_minimal() +
  theme(legend.position = "none")

```

### Echo state network setup

#### Transduction (sequence-to-sequence model)

We can first solve this task using a sequence-to-sequence encoding approach. For this task, the goal is to predict the speaker for each time step of each utterance.

The first step is to get the data where the label is repeated for each time step. This is easily done with the `repeat_targets` argument as follow :

```{r}
japanese_vowels <- reservoirnet::generate_data(
    dataset = "japanese_vowels",
    repeat_targets=TRUE)$japanese_vowels

X_train <- japanese_vowels$X_train
Y_train <- japanese_vowels$Y_train
X_test <- japanese_vowels$X_test
Y_test <- japanese_vowels$Y_test
```

Then we can train a simple Echo State Network to solve this task. Here we will connect both the input layer and the reservoir layer to the readout layer.

```{r}
source <- createNode("Input")
readout <- createNode("Ridge",ridge=1e-6)
reservoir <- createNode("Reservoir",units = 500,lr=0.1, sr=0.9)

model <- list(source %>>% reservoir, source) %>>% readout
```

We can then fit the model and predict the labels for the test data :

```{r}
model_fit <- reservoirnet::reservoirR_fit(node = model,
                                          X = X_train,
                                          Y = Y_train,
                                          stateful = FALSE,
                                          warmup = 2)

Y_pred <- reservoirnet::predict_seq(node = model_fit$fit,
                                    X = X_test,
                                    stateful = FALSE)
```

From the `Y_pred` and `Y_test` we can represent graphically the forecast for some patients :

```{r fig.cap="Prediction in a sequence-to-sequence approach 6 samples with 3 speakers and 2 utterance each. The higher the score of the speaker, the lighter the color.", fig.height=6}
dfplotseqtoseq <- lapply(vec_sample,
                 FUN = function(i){
                   speaker <- which(Y_test[[i]][1,] == 1)
                   Y_pred[[i]] %>%
                     as.data.frame() %>%
                     tibble::rowid_to_column(var = "Time") %>%
                     tidyr::pivot_longer(cols = -Time,
                                         names_to = "pred_speaker",
                                         values_to = "prediction") %>%
                     mutate(pred_speaker = gsub(x = pred_speaker,
                                                pattern = "V", "")) %>%
                     mutate(speaker = speaker, .before = 1,
                            uterrance = i) %>%
                     return()
                 }) %>%
  bind_rows()

ggplot(dfplotseqtoseq, mapping = aes(x = Time,
                                     y = pred_speaker,
                                     fill = prediction)) +
  geom_tile() +
  facet_wrap(uterrance ~ speaker,
             labeller = label_bquote(cols = "speaker" : .(speaker)),
             ncol = 2) +
  theme_minimal() +
  labs(y = 'Predicted speaker',
       fill = "Prediction score")
```

For those 6 utterances, the model correctly identify the speaker for most of the time steps. We can then evaluate the overall accuracy of the model :

```{r}
accuracy <- function(pred, truth) mean(pred == truth)

Y_pred_class <- sapply(Y_pred, FUN = function(x) apply(as.matrix(x),
                                                       1,
                                                       which.max))
Y_test_class <- sapply(Y_test, FUN = function(x) apply(as.matrix(x),
                                                       1,
                                                       which.max))
score <- accuracy(array(unlist(Y_pred_class)), array(unlist(Y_test_class)))

print(paste0("Accuracy: ", round(score * 100,3) ,"%"))
```

### Classification (sequence-to-vector model)

Another approach is the sequence-to-vector encoding. For this task we aim to predict the speaker of the whole utterance instead of the speaker of each time step (i.e the label is assign to the whole sequence).

```{r}
japanese_vowels <- reservoirnet::generate_data(
    dataset = "japanese_vowels")$japanese_vowels

X_train <- japanese_vowels$X_train
Y_train <- japanese_vowels$Y_train
X_test <- japanese_vowels$X_test
Y_test <- japanese_vowels$Y_test
```

In this example, we only connect the reservoir to the output layer.

```{r}
reservoir <- reservoirnet::createNode("Reservoir", units = 500, lr=0.1, sr=0.9)
readout <- reservoirnet::createNode("Ridge",ridge=1e-6)
```

To perform this task, we need to modify the training and testing process. Thanks to the echo property of RC, the information of previous time step is stored in the reservoir (i.e RC have a "memory"). Therefore, the last vector of states accumulates information from all previous states. For this task of sequence-to-vector encoding we only use this last state. This performed as follow :

```{r}
states_train = list()
k <- 1
for (x in X_train) {
  states <- reservoirnet::predict_seq(node = reservoir, X = x,
                                      reset=TRUE)
  states_train[[k]] <- t(as.matrix(states[nrow(states),]))
  k <- k+1
}
```

Then we can train the readout based on this last state vector. In that case, `Y_train`

We can now train the readout only on the last state vectors. Here `Y_train` contains a single label for each utterance.

```{r}
res <- reservoirnet::reservoirR_fit(readout,X = states_train, Y = Y_train)
```

The prediction is also modified to use only the final state :

```{r}
Y_pred <- list()
k <- 1
for (x in X_test) {
  states <- reservoirnet::predict_seq(node = reservoir, X = x,
                                      reset=TRUE)
  y <- reservoirnet::predict_seq(node = readout,
                                 X = as.array(states[nrow(states),]))
  Y_pred[[k]] <- y
  k <- k+1
}
```

```{r fig.cap="Prediction in a sequence-to-sequence approach 6 samples with 3 speakers and 2 utterance each. The higher the score of the speaker, the lighter the color."}
dfplotseqtovec <- lapply(vec_sample,
                 FUN = function(i){
                   speaker <- which(Y_test[[i]][1,] == 1)
                   Y_pred[[i]] %>%
                     as.data.frame() %>%
                     tidyr::pivot_longer(cols = everything(),
                                         names_to = "pred_speaker",
                                         values_to = "prediction") %>%
                     mutate(pred_speaker = gsub(x = pred_speaker,
                                                pattern = "V", "")) %>%
                     mutate(speaker = speaker, .before = 1,
                            uterrance = i,
                            target = speaker == pred_speaker) %>%
                     return()
                 }) %>%
  bind_rows()

ggplot(dfplotseqtovec,
       mapping = aes(x = pred_speaker,
                     y = prediction,
                     color = target)) +
  geom_point() +
  facet_wrap(uterrance ~ speaker,
             labeller = label_bquote(cols = "speaker" : .(speaker)),
             ncol = 2) +
  scale_color_manual(values = c("darkgrey", "red")) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(y = 'Score',
       fill = "Speaker")
```

We observe, for those 6 uterrances that the model is correct for each of them. We can also compute the overall accuracy :

```{r}
Y_pred_class <- sapply(Y_pred,
                       FUN = function(x) apply(as.matrix(x),1,which.max))
Y_test_class <- sapply(Y_test,
                       FUN = function(x) apply(as.matrix(x),1,which.max))

score <- accuracy(pred = Y_test_class, truth = Y_pred_class)

print(paste0("Accuracy: ", round(score * 100,3) ,"%"))
```

## Regression

### Goal

Here you will learn basic use of reservoirnet package. This will be
illustrated with the covid-19 dataset included in the pakage. Those data
contain the hospitalisations, positive RT-PCR and overall RT-PCR from
Santé Publique France available on data.gouv.fr (see `help(dfCovid)` for
more informations). Our goal will be to forecast the number of
hospitalised patients 14 days ahead. To do so, we will first learn on
data prior to 2022-01-01 and forecast on the following data.

We can load the data and set the given task :

```{r}
data("dfCovid")
dist_forecast = 14
traintest_date = as.Date("2022-01-01")
```

### Covid-19 data

Because both RT-PCR have strong variations, we will first proceed by
taking the moving average on the last 7 days of those features. Also we
add an outcome and an `outcomeDate` colum which will be useful to train
the model. Furthermore we will compute the
`outcome_deriv = outcome - hosp` which corresponds to the variation of
hospitalisation compared to the current number of hospitalized patients.

```{r}
dfOutcome <- dfCovid %>%
  # outcome at 14 days
  mutate(outcome = lead(x = hosp, n = dist_forecast),
         outcomeDate = date + dist_forecast,
         outcome_deriv = outcome - hosp) %>%
  # rolling average for tested and positive_pcr
  mutate_at(.vars = c("Positive", "Tested"),
            .funs = function(x) slider::slide_dbl(.x = x,
                                                  .before = 6,
                                                  .f = mean))
```

We can now plot the data :

```{r fig.cap="Hospitalisations, IPTCC and positive PCR of Bordeaux University Hospital."}
dfOutcome %>%
  tidyr::pivot_longer(cols = c("hosp", "Positive", "Tested")) %>%
  ggplot2::ggplot(mapping = aes(x = date, y = value)) +
  geom_line() +
  facet_grid(name ~ ., scales = "free_y") +
  theme_bw() +
  geom_vline(mapping = aes(color = "train-test sets",
                           xintercept = traintest_date)) +
  labs(color = "") +
  theme(legend.position = "bottom")
```

### First reservoir

Setting a reservoir is easily done with the `createNode()` function. The
important hyperparameters are the following :

-   Number of nodes (`units`) : it corresponds to the number of nodes
    inside the reservoir. Usually, the more the better but more nodes
    increase the computation time.
-   Leaking rate (`lr`) : the leaking rate corresponds to the balance
    between the new inputs and the previous state. A leaking rate of 1
    only consider informations from the new inputs.
-   Spectral radius (`sr`): the spectral radius is the maximum absolute
    eigenvalue of the reservoir connectivity matrix. A small spectral
    radius induces stable dynamics inside the reservoir, a high spectral
    radius induces chaotic regimen inside the reservoir.
-   Input scaling (`input_scaling`): the input scaling is a gain applied
    to the input features of the reservoir.

For this part of the tutorial, we will set the hyperparameter at a given
value. Hyperparameter opitmisation will be detailed at section [Study
case](#studycase).

```{r}
reservoir <- reservoirnet::createNode(nodeType = "Reservoir",
                                      seed = 1,
                                      units = 500,
                                      lr = 0.7,
                                      sr = 1,
                                      input_scaling = 1)
```

Then we can feed the data to the reservoir and see the activation
depending on time of the reservoir. To do so, we first prepare the data
and transform it to an array.

```{r}
## select explanatory and transform it to an array
X <- dfOutcome %>%
  filter(outcomeDate < traintest_date) %>%
  select(hosp, Positive, Tested) %>%
  as.matrix() %>%
  as.array()
```

Then we run the `predict_seq` function. The parameter reset is set to
TRUE to be sure that the node is clear from data (it is optional here).

```{r}
reservoir_state <- predict_seq(node = reservoir, X = X, reset = TRUE)
```

Now we can visualise node activation using the `plot` function :

```{r fig.cap="Node acivation over time."}
plot(reservoir_state)
```

We see a lot of nodes with a steady state. It will be really difficult
for the ridge output layer to learn on those nodes because they present
no information. The problem is caused by the different scales of the
features, to take this into account, we can divide them by their maximum
value to scale each of them between `-1` and `1` :

```{r}
stand_max <- function(x) return(x/max(x))
# scaled features
Xstand <- dfOutcome %>%
  filter(date < traintest_date) %>%
  select(hosp, Positive, Tested) %>%
  mutate_all(.funs = stand_max) %>%
  as.matrix() %>%
  as.array()
```

We then feed them to the reservoir and plot the node activation :

```{r fig.cap="Node acivation over time. Scaled features"}
# feed them to the reservoir
reservoir_state_stand <- predict_seq(node = reservoir,
                                     X = Xstand,
                                     reset = TRUE)
# plot the output
plot(reservoir_state_stand)
```

The reservoir dynamics now look much better as no node seem to get
saturated.

### Forecast

In order to train the reservoir, we should train the last layer which
linearly combines the neuron's output.

#### Set the ESN

First we define this last layer with a ridge penalty of `1e3`. This
hyperparameter can also be optimised which will be discussed at section
[Study case](#studycase).

```{r}
readout <- reservoirnet::createNode(nodeType = "Ridge",
                                    ridge = 1e3)
```

And we connect it to the previously defined reservoir :

```{r}
model <- reservoirnet::link(reservoir, readout)
```

The model is now ready we must now train the last layer.

#### Set the data

First we separate the train set on which we will learn the ridge
coefficients and the test set on which we will make the forecast. We
define the train set to be all the data before 2022-01-01 and the test
data to be all the data to have forecast both on train and test set.

```{r}
# train set
yTrain <- dfOutcome %>%
  filter(outcomeDate <= traintest_date) %>%
  select(outcome)
yTrain_variation <- dfOutcome %>%
  filter(outcomeDate <= traintest_date) %>%
  select(outcome_deriv)
xTrain <- dfOutcome %>%
  filter(outcomeDate <= traintest_date) %>%
  select(hosp, Positive, Tested)
# test set
xTest <- dfOutcome %>%
  select(hosp, Positive, Tested)
```

We now standardise with the same formula as seen before. We learn the
standardisation on the training set :

```{r}
# standardise based on training set values
ls_fct_stand <- apply(xTrain,
                      MARGIN = 2,
                      FUN = function(x) function(feature) return(feature/(max(x))))
```

And we apply it to both the train and test sets :

```{r results=FALSE}
xTrainstand <- xTrain
xTeststand <- xTest
lapply(X = names(ls_fct_stand),
       FUN = function(x){
         xTrainstand[,x] <<- ls_fct_stand[[x]](feature = xTrain[,x])
         xTeststand[,x] <<- ls_fct_stand[[x]](feature = xTest[,x])
         return()
       })
```

Finally we convert all those data to array :

```{r}
# convert to array
lsdf <- lapply(list(yTrain = yTrain,
                    yTrain_variation = yTrain_variation,
                    xTrain = xTrainstand,
                    xTest = xTeststand),
               function(x) as.array(as.matrix(x)))
```

#### Train the model and predict

Now, the easy part. We are going to train the reservoir with the train
set. To do so, we set a warmup of 30 days during which the data are
propagating into the reservoir but not used to fit the ridge layer.

```{r}
### train the reservoir ridge output
fit <- reservoirnet::reservoirR_fit(node = model,
                                    X = lsdf$xTrain,
                                    Y = lsdf$yTrain,
                                    warmup = 30,
                                    reset = TRUE)
```

Now that the ridge layer is trained, we can perform predictions as seen
before. We set the parameter `reset` to TRUE in order to clean the
reservoir from the data used by the training set.

```{r}
### predict with the reservoir
vec_pred <- reservoirnet::predict_seq(node = fit$fit,
                                      X = lsdf$xTest,
                                      reset = TRUE)
```

```{r fig.cap="Forecast"}
dfOutcome %>%
  mutate(pred = vec_pred) %>%
  na.omit() %>%
  ggplot(mapping = aes(x = outcomeDate)) +
  geom_line(mapping = aes(y = outcome,
                          color = "observed")) +
  geom_line(mapping = aes(y = pred,
                          color = "forecast")) +
  geom_vline(mapping = aes(color = "train-test sets",
                           xintercept = traintest_date)) +
  scale_color_manual(values = c("#3772ff", "#080708", "#df2935")) +
  theme_bw() +
  labs(color = "", x = "Date", y = "Hospitalisations")
```

We observe that the model forecast is not fully accurate, both on the
test set and the train set. In that case, one option could be to reduce
ridge penalisation to fit more closely the data, the optimisation of
ridge hyperparameter will be discussed at section [Study
case](#studycase). Another possibility is to ease the learning of the
algorithm by forecasting the variation of the hospitalisation instead of
the number of hospitalised patients. For that step, we will learn on the
`outcome_deriv` contained in `yTrain_variation` data which is defined
outcome as `outcome_deriv = outcome - hosp`.

```{r fig.cap="Covid-19 hospitalisations forecast. The model is either trained to forecast the number of hospitalisations (denoted Raw) or the variation of the hospitalisations compared to current level of hospitalisation (denoted Variation)"}
## Fit reservoir on outcome variation instead of raw outcome
fit2 <- reservoirnet::reservoirR_fit(node = model,
                                     X = lsdf$xTrain,
                                     Y = lsdf$yTrain_variation,
                                     warmup = 30,
                                     reset = TRUE)
## Get the forecast on the test set
vec_pred2_variation <- reservoirnet::predict_seq(node = fit2$fit,
                                                 X = lsdf$xTest,
                                                 reset = TRUE)
## Transform the outome variation forecast into hospitalisation forecast
vec_pred2 <- vec_pred2_variation + xTest$hosp
## Plot the results
dfOutcome %>%
  mutate(Raw = vec_pred,
         Variation = vec_pred2) %>%
  tidyr::pivot_longer(cols = c(Raw, Variation),
                      names_to = "Outcome_type",
                      values_to = "Forecast") %>%
  na.omit() %>%
  ggplot(mapping = aes(x = outcomeDate)) +
  geom_line(mapping = aes(y = outcome,
                          color = "observed")) +
  geom_line(mapping = aes(y = Forecast,
                          color = "Forecast")) +
  geom_vline(mapping = aes(color = "train-test sets",
                           xintercept = traintest_date)) +
  facet_wrap(Outcome_type ~ .,
             labeller = label_bquote(cols = "Outcome" : .(Outcome_type))) +
  scale_color_manual(values = c("#3772ff", "#080708", "#df2935")) +
  theme_minimal() +
  labs(color = "", x = "Date", y = "Hospitalisations")
```

We observe an improvement of the forecast compared to the forecast of
hospitalisation previously discussed. From there, many improvement can
be implemented including : frequent update of the model, leverage
additional features and hyperparameter optimisation. This notions will
be discussed in the next [section](#studycase).

# Study case: Prediction covid-19 {#studycase}

what is covid-19 and covid-19 pandemic

hospital is a central actor during pandemic, importance to anticipate
high number of patients

several models tried to forecast hospitalisations but task is difficult
and many models struggled to perform well.

In this setting, reservoir computing seem to be an appropriate approach
regarding the task.

## Study case

This use case is based on data from Bordeaux University Hospital. During
the pandemic a predictive model was elaborate and regularly use to
forecast 14 days ahead the number of hospitalized patients at the
hospital. In this work we will compare the first approach based on
linear regression with elastic-net penalisation to reservoir computing
approach.

## Evaluation framework

### Train and test set

The task was to forecast 14 days ahead the number of hospitalised
patients. The dataset was separated into two periods. First period from
xx to xx served to identify relevant hyperparameters. Second period
after xx was used to evaluate the model performance. As the time serie
is not stationary and most appropriate hyperparameter value might change
over time, we also investigated the effect of hyperparameter update
every month.

The performance of the model was evaluated according to several metrics:
the median absolute error, the median relative error, the absolute error
relative to baseline, the relative error relative to baseline. For each
metric, the median was chosen over the mean to be robust to outliers.
For instance, for relative error, when the outcome is low (e.g equal to
1 hospitalisation), an error of 5 hospitalisations could results to a
500% relative error.

Median absolute error (AE): Median relative error (RE):

-   Baseline model (forecast at T+14 = observed at T)
-   Metrics

## Hyperparameter optimisation using GA

-   Recall hyperparameters
-   Present GA algorithm + explain why it is important here (large
    number of hp + assumption free compared to other algorithm, cons =
    expensive)

## Present results from GA

-   Present results and detail them

## Present forecast performance

## Discussion on use case

# Previous work

Those work were done previously, the setting of train/test and
hyperparameter optimisation were different and probably they should be
done again in a similar setting for relevant comparison.

## Raw outcome, first derivative or second derivative ?

See
<https://github.com/thomasferte/PredictCovid/blob/esn_update_report/reporting/02_esn/01_explore_esn_prediction/00_reporting/explore_esn.md>

## Raw feature or scaled feature ?

See
<https://github.com/thomasferte/PredictCovid/blob/esn_update_report/reporting/02_esn/01_explore_esn_prediction/00_reporting/explore_esn.md>

## Smoothed or not smoothed

See
<https://github.com/thomasferte/PredictCovid/blob/esn_update_report/reporting/02_esn/01_explore_esn_prediction/00_reporting/explore_esn.md>

## Number of nodes

See
<https://github.com/thomasferte/PredictCovid/blob/esn_update_report/reporting/02_esn/01_explore_esn_prediction/00_reporting/explore_esn_r_pipeline.md>

## Use of second derivative among the interesting features

See
<https://github.com/thomasferte/PredictCovid/blob/esn_update_report/reporting/02_esn/01_explore_esn_prediction/00_reporting/tune_esn2.md>

## Results of random sampling with R

See
<https://github.com/thomasferte/PredictCovid/blob/esn_update_report/reporting/02_esn/01_explore_esn_prediction/meeting_1801/master_report.md>

# Discussion and conclusion

# Bibliography
