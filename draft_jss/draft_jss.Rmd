---
documentclass: jss
bibliography: article.bib
author:
  - name: FirstName LastName
    orcid: 0000-0000-0000-0000
    affiliation: University/Company
    # use this syntax to add text on several lines
    address: |
      | First line
      | Second line
    email: \email{name@company.com}
    url: https://posit.co
  - name: Second Author
    orcid: 0000-0000-0000-0000
    affiliation: 'Affiliation \AND'
    # To add another line, use \AND at the end of the previous one as above
  - name: Third Author
    orcid: 0000-0000-0000-0000
    affiliation: 'Affiliation'
    # To add another line, use \AND at the end of the previous one as above
  - name: Third Author
    orcid: 0000-0000-0000-0000
    affiliation: 'Affiliation \AND'
    # To add another line, use \AND at the end of the previous one as above
  - name: Third Author
    orcid: 0000-0000-0000-0000
    affiliation: 'Affiliation'
    # To add another line, use \AND at the end of the previous one as above
  - name: Fourth Author
    orcid: 0000-0000-0000-0000
    address: |
      | Department of Statistics and Mathematics,
      | Faculty of Biosciences,
      | Universitat Autònoma de Barcelona
    affiliation: |
      | Universitat Autònoma 
      | de Barcelona
    # use a different affiliation in adress field (differently formated here)
    affiliation2: Universitat Autònoma de Barcelona
title:
  formatted: "Reservoir computing in R : a tutorial using \\pkg{reservoirnet}"
  # If you use tex in the formatted title, also supply version without
  plain:     "Reservoir computing in R : a tutorial using reservoirnet"
  # For running headers, if needed
  short:     "\\pkg{reservoirnet}: reservoir computing in R"
abstract: >
  The abstract of the article.
keywords:
  # at least one keyword must be supplied
  formatted: [keywords, not capitalized, "\\proglang{Java}"]
  plain:     [keywords, not capitalized, Java]
preamble: >
  \usepackage{amsmath}
output:
  bookdown::pdf_book:
    base_format: rticles::jss_article
  bookdown::word_document2:
    default
knit: (function(inputFile, encoding){
  rmarkdown::render(inputFile, encoding = encoding,
  output_dir = here::here("draft_jss"), output_format = "all") })
---

```{r, setup, include=FALSE}
options(prompt = 'R> ', continue = '+ ')
set.seed(1)
```

# Introduction

## Reservoir networks presentation

Reservoir Computing (RC) is a prominent machine learning methodology
that has gained significant attention in recent years for its ability to
effectively process information generated by dynamical systems. This
innovative approach leverages the dynamics of a high-dimensional
reservoir to perform complex computations and solve various tasks based
on the response of this dynamical system to input signals. Reservoir
Computing is characterized by its unique architecture, which includes a
dynamic system, often referred to as a reservoir, that projects temporal
input signals onto a high-dimensional feature space. This
high-dimensional representation of input data enables Reservoir
Computing to excel in tasks such as time series prediction, pattern
recognition, and signal processing.

**TODO : cite : Jaeger, Herbert. “The “echo state” approach to analysing and training recurrent neural networks-with an erratum note.” Bonn, Germany: German National Research Center for Information Technology GMD Technical Report 148.34 (2001): 13.**
**Maass W, Natschläger T, Markram H. Real-time computing without stable states: a new framework for neural computation based on perturbations. Neural Comput. 2002;14(11):2531‐2560.**

## Why reservoir are interesting compared to standard approaches (penalised regression) and deep learning approaches?

RC exhibits several advantages over its counterparts. It excels in tasks
that require the processing of temporal data, such as time series
forecasting and speech recognition, thanks to its inherent memory
capabilities. RC offers computational efficiency and reduced susceptibility
to overfitting compared to deep learning. Moreover, RC is able to capture intricate temporal
patterns compared to statistical methods such as penalized regression.

**TODO : find ref to support it.**

## examples of previous use

**TODO: add ref from use case intro on use in epidemiology + find review**
**Table 1 contains a list of use case of RC : https://www.sciencedirect.com/science/article/pii/S0893608019300784**

## challenges of using reservoir

RC use is still challenging for several reasons. First, its stochastic
nature due to the reservoir random part makes the results of a given RC
unpredictable even if the hyperparameters are well chosen. Second,
even though, RC relies on several hyperparameter to determine the
connections between neurons. There exist few guidance about the way to
set the hyperparameters and most often the user must rely on a wrapper
approach were combinations of hyperparameters performance are evaluated
on a train set before choosing the ideal combination one. Third, most
published paper are based on a low dimensional setting and few guidance
exist in the context of high dimensional data, especially considering
the choice of hyperparameters. Fourth, the use of reservoir computing in
an epidemic setting with highly non-stationary time series is scarce
whereas it is of tremendous interest for epidemiologists. Fifth, there
is currently no implementation available on R making the use of this
method challenging for user not familiar with Python.

## what we do

In this paper we propose to explore those challenges with extensive
guidance in order to help new user to fully benefit from RC. First a
general introduction to reservoir computing is provided with a tutorial
for its use using `reservoirnet`, a R package based on `reservoirPy` Python
module. Second, we explore the different challenges of the use of RC in
a non-stationary high dimensional setting based on the use case of
covid-19 hospitalizations forecast with extensive guidance on the
modelling strategy, the choice of hyperparameters using a genetic
algorithm and the implementation.

# RC presentation

RC is a machine learning algorithm composed of an input layer denoted
$u(t)$ which is randomly transformed by the reservoir into an activation
state denoted $x(t)$ which is fed to a ridge penalized linear regression
trained to forecast the outcome denoted as $y(t)$ as depicted at figure
\@ref(fig:rcpresentation).

```{r rcpresentation, echo=FALSE, fig.cap="my caption", out.width = '90%', fig.cap="Reservoir computing is composed of an input layer, a reservoir and an output layer. Connection between input layer and reservoir and inside reservoir are random. Only the output layer is optimized based on a ridge penalized linear regression."}
knitr::include_graphics("images/schema.png/image1.png")
```

The input layer $u(t)$ is an $M$-dimension vector, where $M$ is the number of input time series, which corresponds to the values of the input time series at time $t$ where $t = 1, …, T$. The reservoir layer $x(t)$ is an $N_{res}$-dimensional vector where $N_{res}$ is the number of nodes in the reservoir. The value $x(t)$ is defined as follow:

$$x( t+1 ) = ( 1 - \alpha )  x ( t) + \alpha \: tanh( W x(t) + W_{in} u(t+1) ) \text{ , where } \alpha \in [0, 1 ]$$

The leaking rate alpha define the rate of update of the nodes. The
closer $\alpha$ is to $1$, the higher the reservoir is sensitive to new
inputs (i.e $u(t)$). Therefore, the reservoir state at time $t+1$
denoted $x(t+1)$ depends on the reservoir state at the previous time
(i.e $x(t)$) and the new inputs (i.e $u(t+1)$). Both $W_{in}$ and $W$
are random matrices of size $Nres \times M$ and $Nres \times Nres$
respectively.

$W_{in}$ is a dense matrix generated using a Bernoulli distribution
where each value can be either $-I_{scale}(m)$ or $I_{scale}(m)$ with an
equal probability where $m = 1, …, M$ corresponds to a given feature in
the input layer. The input scaling, denoted $I_{scale}$, is an
hyperparameter coefficient which can be common to all features from the
input layer or specific to each feature $m$. In that case, the more
important the feature is, the greater should be its input scaling. $W$
is a sparse matrix where values are generated from a Gaussian
distribution $\mathcal{N}(0,1)$. Then the matrix $W$ is scaled
according to the spectral radius, an hyperparameter defining the highest
eigen value of $W$.

The final layer is a linear regression with ridge penalization where the
explanatory features are the reservoir state and the variable to be
explain is the outcome to predict such that:

$$W_{out} = YX^T ( XX^T + \lambda  I)^{ -1 }$$

Where x(t) and y(t) are accumulated in X and Y respectively such that:

$$X = \begin{bmatrix} x(1) \\ x(2) \\ ... \\ x(T) \end{bmatrix}
\text{ and } Y = \begin{bmatrix} y(1) \\ y(2) \\ ... \\ y(T) \end{bmatrix}$$

The parameter $\lambda$ is the ridge penalization which aims to prevent
overfitting.

Overall, there are four main hyperparameters to be chosen by the user: the leaking rate which defines the memory of the RC, the input scaling which define the relative importance of the features, the spectral radius which define the connections of the neurons inside the reservoir which in turn define the degree of non-linear combination of features and the ridge penalization which controls the degree of overfitting. The choice of hyperparameter is difficult and the use of a wrapper approach where the performance of the RC with different combinations of hyperparameters is evaluated on a train set, the best combination is chosen for the use on the test set.

# Basic package use

In this section, we will cover the basics of reservoirnet use including
installation, classification and regression. We will provide brief
overview of the package use and more in depth description is provided in
section 4 with the covid-19 forecast use case.

## Installation

reservoirnet is an R package api making the python module reservoirPy
easily callable from R. It is available on CRAN (see
<https://cran.r-project.org/package=reservoirnet>) and can be installed
using:

```{r eval=FALSE}
install.packages("reservoirnet")
```

Reservoir Computing (RC) is well suited to both regression and
classification tasks. We will introduce a simple example for both task.

## Regression {#basicregression}

### Covid-19 data

In this first use case, we will introduce the fundamental usage of the `reservoirnet` package. This demonstration will be conducted using the COVID-19 dataset that is included within the package. These data encompass hospitalization, positive RT-PCR results, and overall RT-PCR data sourced from Santé Publique France, which are publicly available on data.gouv.fr (for further details, refer to `help(dfCovid)`). Our primary objective is to predict the number of hospitalized patients 14 days into the future. To accomplish this, we will initially train our model on data preceding the date of January 1, 2022, and subsequently apply it to forecast values using the subsequent dataset.

We can proceed by loading useful packages (i.e ggplot2 [@wickham_ggplot2_2016] and dplyr [@wickham_dplyr_2023], **TODO : add cowplot**), data and define the task:

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(reservoirnet)
library(cowplot)
library(patchwork)

data("dfCovid")
dist_forecast = 14
traintest_date = as.Date("2022-01-01")
```

Due to the substantial fluctuations observed in both RT-PCR metrics, our initial step involves applying a moving average computation over the most recent 7-day periods for these features. Additionally, we augment the dataset by introducing an `outcome` column and an `outcomeDate` column, which will serve as valuable inputs for model training. Moreover, we calculate the `outcome_deriv` as the difference between the outcome and the number of hospitalized patients (`hosp`), representing the variation in hospitalization in relation to the current count of hospitalized individuals. The resulting smoothed data is visualized in Figure \@ref(fig:covidintro).

```{r}
dfOutcome <- dfCovid %>%
  # outcome at 14 days
  mutate(outcome = lead(x = hosp, n = dist_forecast),
         outcomeDate = date + dist_forecast,
         outcome_deriv = outcome - hosp) %>%
  # rolling average for tested and positive_pcr
  mutate_at(.vars = c("Positive", "Tested"),
            .funs = function(x) slider::slide_dbl(.x = x,
                                                  .before = 6,
                                                  .f = mean))
```

```{r covidintro, fig.cap="Hospitalizations, IPTCC and positive PCR of Bordeaux University Hospital.", echo = FALSE}
dfOutcome %>%
  tidyr::pivot_longer(cols = c("hosp", "Positive", "Tested")) %>%
  ggplot2::ggplot(mapping = aes(x = date, y = value)) +
  geom_line() +
  facet_grid(name ~ ., scales = "free_y") +
  theme_bw() +
  geom_vline(mapping = aes(color = "train-test sets",
                           xintercept = traintest_date)) +
  labs(color = "") +
  theme(legend.position = "bottom")
```

### First reservoir

Setting a reservoir is done with the `createNode()` function. The important hyperparameters are the following :

-   Number of nodes (`units`) : it corresponds to the number of nodes inside the reservoir. Usually, the more the better but more nodes increases the computation time.
-   Leaking rate (`lr`) : the leaking rate corresponds to the balance between the new inputs and the previous state. A leaking rate of 1 only consider information from new inputs.
-   Spectral radius (`sr`): the spectral radius is the maximum absolute eigenvalue of the reservoir connectivity matrix. A small spectral radius induces stable dynamics inside the reservoir, a high spectral radius induces chaotic regimen inside the reservoir.
-   Input scaling (`input_scaling`): the input scaling is a gain applied to the input features of the reservoir.
-   Seed (`seed`): because the reservoir connections are set at random, setting the seed is a good approach to ensure   reproducibility. Another approach would be to use several different reservoir but it increases computation time.

For this part of the tutorial, we will set the hyperparameter at a given value. Hyperparameter optimization will be detailed at section [Study case](#studycase).

```{r}
reservoir <- reservoirnet::createNode(nodeType = "Reservoir",
                                      seed = 1,
                                      units = 500,
                                      lr = 0.7,
                                      sr = 1,
                                      input_scaling = 1)
```

Then we can feed the data to the reservoir and see the activation depending on time of the reservoir. To do so, we first prepare the data and transform it to a matrix :

```{r}
## select explanatory and transform it to an array
X <- dfOutcome %>%
  filter(outcomeDate < traintest_date) %>%
  select(hosp, Positive, Tested) %>%
  as.matrix()
```

Then we run the `predict_seq` function. It takes as input a node (i.e a reservoir or a reservoir associated with an output layer) and the feature matrix.

```{r}
reservoir_state <- predict_seq(node = reservoir, X = X)
```

Now we can visualize node activation using the `plot` function presented at figure \@ref(fig:nodeactivationbad).

```{r nodeactivationbad, fig.cap="Node activation over time."}
plot(reservoir_state)
```

Numerous nodes within the system exhibit a consistent equilibrium state. The challenge arises when the ridge output layer attempts to acquire knowledge from these nodes, as they do not convey meaningful information. This issue can be attributed to the disparate scales of the features. To address this concern, a practical approach involves normalizing the features by dividing each of them by their respective maximum values, thereby scaling them within the range of `-1` to `1`.

```{r}
stand_max <- function(x) return(x/max(x))
# scaled features
Xstand <- dfOutcome %>%
  filter(date < traintest_date) %>%
  select(hosp, Positive, Tested) %>%
  mutate_all(.funs = stand_max) %>%
  as.matrix() %>%
  as.array()
```

We then feed them to the reservoir and plot the node activation again. Compared to \@ref(fig:nodeactivationbad), the obtained node activation \@ref(fig:nodeactivationgood) shows interesting trend outputs as no node seems saturated.

```{r nodeactivationgood, fig.cap="Node acivation over time. Scaled features"}
# feed them to the reservoir
reservoir_state_stand <- predict_seq(node = reservoir,
                                     X = Xstand,
                                     reset = TRUE)
# plot the output
plot(reservoir_state_stand)
```

### Forecast

In order to train the reservoir, we should train the last layer which
linearly combines the neuron's output.

#### Set the ESN

Initially, we establish the output layer, incorporating a ridge penalty set at `1e3`. It's important to note that this hyperparameter can be subject to optimization, a topic that will be explored in the forthcoming [Study Case](#studycase) section. This parameter plays a pivotal role in fine-tuning the model's conformity to the data. When set excessively high, the risk of underfitting arises, whereas setting it too low can lead to overfitting. We connect the output layer to the reservoir making the model ready to be trained.

```{r}
readout <- reservoirnet::createNode(nodeType = "Ridge",
                                    ridge = 1e3)
model <- reservoirnet::link(reservoir, readout)
```

#### Set the data

First we separate the train set on which we will learn the ridge coefficients and the test set on which we will make the forecast. We define the train set to be all the data before 2022-01-01 and the test data to be all the data to have forecast both on train and test sets.

```{r}
# train set
dftrain <- dfOutcome %>% filter(outcomeDate <= traintest_date)
yTrain <- dftrain %>% select(outcome)
yTrain_variation <- dftrain %>% select(outcome_deriv)
xTrain <- dftrain %>% select(hosp, Positive, Tested)
# test set
xTest <- dfOutcome %>% select(hosp, Positive, Tested)
```

We standardize with the same formula as seen before. We learn the standardization on the training set and apply it on the test set. Then we convert the dataframe to matrix.

```{r results=FALSE}
# copy train and test sets
xTrainstand <- xTrain
xTeststand <- xTest
# standardise based on training set values
ls_fct_stand <- apply(xTrain,
                      MARGIN = 2,
                      FUN = function(x) function(feature) return(feature/(max(x))))
lapply(X = names(ls_fct_stand),
       FUN = function(x){
         xTrainstand[,x] <<- ls_fct_stand[[x]](feature = xTrain[,x])
         xTeststand[,x] <<- ls_fct_stand[[x]](feature = xTest[,x])
         return()
       })
# convert to array
lsdf <- lapply(list(yTrain = yTrain,
                    yTrain_variation = yTrain_variation,
                    xTrain = xTrainstand,
                    xTest = xTeststand),
               function(x) as.matrix(x))
```

#### Train the model and predict

We then feed the reservoir with the train set. To do so, we set a `warmup` of `30` days during which the data are propagating into the reservoir but not used to fit the output layer.

```{r}
### train the reservoir ridge output
fit <- reservoirnet::reservoirR_fit(node = model,
                                    X = lsdf$xTrain,
                                    Y = lsdf$yTrain,
                                    warmup = 30,
                                    reset = TRUE)
```

Now that the ridge layer is trained, we can forecast. We set the parameter `reset` to `TRUE` in order to clean the reservoir from the data used by the training set.

```{r}
vec_pred <- reservoirnet::predict_seq(node = fit$fit,
                                      X = lsdf$xTest,
                                      reset = TRUE)
```

```{r fig.cap="Forecast"}
dfOutcome %>%
  mutate(pred = vec_pred) %>%
  na.omit() %>%
  ggplot(mapping = aes(x = outcomeDate)) +
  geom_line(mapping = aes(y = outcome,
                          color = "observed")) +
  geom_line(mapping = aes(y = pred,
                          color = "forecast")) +
  geom_vline(mapping = aes(color = "train-test sets",
                           xintercept = traintest_date)) +
  scale_color_manual(values = c("#3772ff", "#080708", "#df2935")) +
  theme_bw() +
  labs(color = "", x = "Date", y = "Hospitalizations")
```

We observe that the model forecast is not fully accurate, both on the test set and the train set. In that case, one option could be to reduce ridge penalization to fit more closely the data, the optimization of ridge hyperparameter will be discussed at section [Study case](#studycase). Another possibility is to ease the learning of the algorithm by forecasting the variation of the hospitalization instead of the number of hospitalized patients. For that step, we will learn on the `outcome_deriv` contained in `yTrain_variation` data which is defined outcome as `outcome_deriv = outcome - hosp`.

```{r fig.cap="Covid-19 hospitalizations forecast. The model is either trained to forecast the number of hospitalizations (denoted Raw) or the variation of the hospitalizations compared to current level of hospitalisation (denoted Variation)"}
## Fit reservoir on outcome variation instead of raw outcome
fit2 <- reservoirnet::reservoirR_fit(node = model,
                                     X = lsdf$xTrain,
                                     Y = lsdf$yTrain_variation,
                                     warmup = 30,
                                     reset = TRUE)
## Get the forecast on the test set
vec_pred2_variation <- reservoirnet::predict_seq(node = fit2$fit,
                                                 X = lsdf$xTest,
                                                 reset = TRUE)
## Transform the outome variation forecast into hospitalisation forecast
vec_pred2 <- vec_pred2_variation + xTest$hosp
## Plot the results
dfOutcome %>%
  mutate(Raw = vec_pred,
         Variation = vec_pred2) %>%
  tidyr::pivot_longer(cols = c(Raw, Variation),
                      names_to = "Outcome_type",
                      values_to = "Forecast") %>%
  na.omit() %>%
  ggplot(mapping = aes(x = outcomeDate)) +
  geom_line(mapping = aes(y = outcome,
                          color = "observed")) +
  geom_line(mapping = aes(y = Forecast,
                          color = "Forecast")) +
  geom_vline(mapping = aes(color = "train-test sets",
                           xintercept = traintest_date)) +
  facet_wrap(Outcome_type ~ .,
             labeller = label_bquote(cols = "Outcome" : .(Outcome_type))) +
  scale_color_manual(values = c("#3772ff", "#080708", "#df2935")) +
  theme_minimal() +
  labs(color = "", x = "Date", y = "Hospitalizations")
```

We observe an improvement of the forecast compared to the forecast of hospitalization previously discussed. From there, many improvement can be implemented including : frequent update of the model, leverage additional features and hyperparameter optimization. This notions will be discussed in the [Study case section](#studycase).

## Classification

### The Japanese vowel dataset

This example is largely inspired from the [classification tutorial of reservoirpy](https://github.com/reservoirpy/reservoirpy/blob/master/tutorials/5-Classification-with-RC.ipynb). To illustrate the classification task, we will use the Japanese vowel dataset (@kudo_multidimensional_1999). The data can be loaded from `reservoirnet` as follow :

```{r}
japanese_vowels <- reservoirnet::generate_data(dataset = "japanese_vowels")[[1]]
X_train <- japanese_vowels$X_train
Y_train <- japanese_vowels$Y_train
X_test <- japanese_vowels$X_test
Y_test <- japanese_vowels$Y_test
```

The dataset comprises 640 vocalizations of the Japanese vowel \ae, contributed by nine distinct speakers. Each vocalization represents a time series spanning between 7 and 29 time steps, encoded as a 12-dimensional vector denoting the Linear Prediction Coefficients (LPC). A visual representation of six distinct utterances from the test set, originating from three different speakers, is depicted in Figure \@ref(fig:vowelpresentation).

```{r vowelpresentation, fig.cap="Vowel dataset, sample with 3 speakers and 2 utterance each.", echo = FALSE}
vec_sample <- c(1, 2, 41, 42, 71, 72)
dfplot_vowel <- lapply(vec_sample,
                 FUN = function(i){
                   speaker <- which(Y_test[[i]] == 1)
                   X_test[[i]] %>%
                     as.data.frame() %>%
                     tibble::rowid_to_column(var = "Time") %>%
                     tidyr::pivot_longer(cols = -Time,
                                         names_to = "component",
                                         values_to = "LPC") %>%
                     mutate(speaker = speaker, .before = 1,
                            uterrance = i) %>%
                     return()
                 }) %>%
  bind_rows()

ggplot(dfplot_vowel, mapping = aes(x = Time, y = LPC, color = component)) +
  geom_line() +
  facet_wrap(uterrance ~ speaker,
             labeller = label_bquote(cols = "speaker" : .(speaker)),
             ncol = 2) +
  theme_minimal() +
  theme(legend.position = "none")

```

The primary objective involves the attribution of each utterance to its respective speaker, this is denoted as classification or sequence-to-vector encoding. The secondary objective involves the attribution of each time step of each utterance to its speaker, this is denoted as transduction or sequence-to-sequence encoding.

### Classification (sequence-to-vector model)

The first approach is the sequence-to-vector encoding. For this task we aim to predict the speaker of the whole utterance (i.e the label is assigned to the whole sequence). We first start by creating the reservoir and the output layer.

```{r}
reservoir <- reservoirnet::createNode("Reservoir", units = 500,
                                      lr=0.1, sr=0.9,
                                      seed = 1)
readout <- reservoirnet::createNode("Ridge",ridge=1e-6)
```

To perform this task, we need to modify the training and testing process. Leveraging the inherent echo property of the RC, information from preceding time steps is preserved within the reservoir, effectively endowing the RC with a form of memory. Consequently, the final state vector encapsulates insights gathered from all antecedent states. In the context of the sequence-to-vector encoding task, only this ultimate state is employed. This process is executed as follows:

```{r}
states_train = list()
k <- 1
for (x in X_train) {
  states <- reservoirnet::predict_seq(node = reservoir, X = x,
                                      reset=TRUE)
  states_train[[k]] <- t(as.matrix(states[nrow(states),]))
  k <- k+1
}
```

Then we can train the readout based on this last state vector. In that case, `Y_train` contains a single label for each utterance.

```{r}
res <- reservoirnet::reservoirR_fit(readout,X = states_train, Y = Y_train)
```

The prediction is also modified using only the final state :

```{r}
Y_pred <- list()
k <- 1
for (x in X_test) {
  states <- reservoirnet::predict_seq(node = reservoir, X = x,
                                      reset=TRUE)
  y <- reservoirnet::predict_seq(node = readout,
                                 X = as.array(states[nrow(states),]))
  Y_pred[[k]] <- y
  k <- k+1
}
```

Figure \@ref(fig:seqtovec) shows the prediction for the 6 utterances depicted at figure \@ref(fig:vowelpresentation) where the model correctly identifies the speaker.

```{r seqtovec, fig.cap="Prediction in a sequence-to-sequence approach 6 samples with 3 speakers and 2 utterance each. The speaker to predict is depicted in blue. For each of the 6 utterance, the model correctly identifies the speaker."}
dfplotseqtovec <- lapply(vec_sample,
                 FUN = function(i){
                   speaker <- which(Y_test[[i]][1,] == 1)
                   Y_pred[[i]] %>%
                     as.data.frame() %>%
                     tidyr::pivot_longer(cols = everything(),
                                         names_to = "pred_speaker",
                                         values_to = "prediction") %>%
                     mutate(pred_speaker = gsub(x = pred_speaker,
                                                pattern = "V", "")) %>%
                     mutate(speaker = speaker, .before = 1,
                            uterrance = i,
                            target = speaker == pred_speaker) %>%
                     return()
                 }) %>%
  bind_rows()

ggplot(dfplotseqtovec,
       mapping = aes(x = pred_speaker,
                     y = prediction,
                     fill = target)) +
  geom_bar(stat = "identity") +
  facet_wrap(uterrance ~ speaker,
             labeller = label_bquote(cols = "speaker" : .(speaker)),
             ncol = 2) +
  scale_fill_manual(values = c("#BDBDBD", "#A3CEF1")) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(y = 'Score',
       x = "Speaker")
```

Then, we can also compute the overall accuracy :

```{r}
accuracy <- function(pred, truth) mean(pred == truth)

Y_pred_class <- sapply(Y_pred,
                       FUN = function(x) apply(as.matrix(x),1,which.max))
Y_test_class <- sapply(Y_test,
                       FUN = function(x) apply(as.matrix(x),1,which.max))

score <- accuracy(pred = Y_test_class, truth = Y_pred_class)

print(paste0("Accuracy: ", round(score * 100,3) ,"%"))
```

### Transduction (sequence-to-sequence model)

For this task, the goal is to predict the speaker for each time step of each utterance. The first step is to get the data where the label is repeated for each time step. This is easily done with the `repeat_targets` argument as follow :

```{r}
japanese_vowels <- reservoirnet::generate_data(
    dataset = "japanese_vowels",
    repeat_targets=TRUE)$japanese_vowels
X_train <- japanese_vowels$X_train
Y_train <- japanese_vowels$Y_train
X_test <- japanese_vowels$X_test
Y_test <- japanese_vowels$Y_test
```

Then we can train a simple Echo State Network to solve this task. For this example we will connect both the input layer and the reservoir layer to the readout layer which is performed by the `%>>%` operator :

```{r}
source <- createNode("Input")
readout <- createNode("Ridge",ridge=1e-6)
reservoir <- createNode("Reservoir",units = 500,lr=0.1, sr=0.9, seed = 1)

model <- list(source %>>% reservoir, source) %>>% readout
```

We can then fit the model and predict the labels for the test data. The `reset` parameter is set to `TRUE` to remove information from the reservoir from the training process.

```{r}
model_fit <- reservoirnet::reservoirR_fit(node = model,
                                          X = X_train,
                                          Y = Y_train,
                                          warmup = 2)

Y_pred <- reservoirnet::predict_seq(node = model_fit$fit,
                                    X = X_test,
                                    reset = TRUE)
```

From the `Y_pred` and `Y_test` we represent at figure \@ref(fig:figseqtoseq) the predictions for the same patients as in figure \@ref(fig:vowelpresentation).

```{r figseqtoseq, fig.cap="Prediction in a sequence-to-sequence approach 6 samples with 3 speakers and 2 utterance each. The higher the score of the speaker, the lighter the color.", fig.height=6}
dfplotseqtoseq <- lapply(vec_sample,
                 FUN = function(i){
                   speaker <- which(Y_test[[i]][1,] == 1)
                   Y_pred[[i]] %>%
                     as.data.frame() %>%
                     tibble::rowid_to_column(var = "Time") %>%
                     tidyr::pivot_longer(cols = -Time,
                                         names_to = "pred_speaker",
                                         values_to = "prediction") %>%
                     mutate(pred_speaker = gsub(x = pred_speaker,
                                                pattern = "V", ""),
                            speaker = speaker,
                            uterrance = i,
                            .before = 1) %>%
                     return()
                 }) %>%
  bind_rows()

ggplot(dfplotseqtoseq, mapping = aes(x = Time,
                                     y = pred_speaker,
                                     fill = prediction)) +
  geom_tile() +
  facet_wrap(uterrance ~ speaker,
             labeller = label_bquote(cols = "speaker" : .(speaker)),
             ncol = 2) +
  theme_minimal() +
  labs(y = 'Predicted speaker',
       fill = "Prediction score")
```

For those 6 utterances, the model correctly identify the speaker for most of the time steps. We can then evaluate the overall accuracy of the model :

```{r}
Y_pred_class <- sapply(Y_pred, FUN = function(x) apply(as.matrix(x),
                                                       1,
                                                       which.max))
Y_test_class <- sapply(Y_test, FUN = function(x) apply(as.matrix(x),
                                                       1,
                                                       which.max))
score <- accuracy(array(unlist(Y_pred_class)), array(unlist(Y_test_class)))

print(paste0("Accuracy: ", round(score * 100,3) ,"%"))
```

# Study case: Covid-19 hospitalizations forecast {#studycase}

## Introduction

Since late 2020, millions of cases of SARS-CoV-2 infection have been documented across the globe [@carrat_evidence_2021, @world_health_organisation_who_2020, @noauthor_estimating_2022]. This ongoing pandemic has exerted significant strain on healthcare systems, resulting in a surge in hospitalizations. This surge, in turn, necessitated modifications to the healthcare infrastructure and gave rise to population-wide lockdown measures aimed at preventing the saturation of healthcare facilities [@kim_health_2020, @simoes_organisation_2021, @hubner_surgery_2020]. The capacity to predict the trajectory of the epidemic on a regional scale is of paramount importance for effective healthcare system management.

Numerous COVID-19 forecasting algorithms have been proposed using different methods (e.g ensemble, deep learning, compartmental), yet none has proven entirely satisfactory [@rahimi_review_2021, @cramer_evaluation_2022]. In France, short-term forecasts  with different methods have been evaluated with similar results [@pottier_forecast_2021, @paireau_ensemble_2022, @carvalho_analysis_2021, @mohimont_convolutional_2021]. In this context a machine learning algorithm based on linear regression with elastic-net penalization, leveraging both Electronic Health Records (EHRs) and public data, was implemented at Bordeaux University Hospital [@ferte_benefit_2022]. This model, which aimed at forecasting the number of hospitalized patients at 14 days, showed good performance but struggled to accurately anticipate dynamic shifts of the epidemic.

RC has been used in the context of covid-19 epidemic forecast [@ghosh_reservoir_2021, @kmet_bezier_2019, @liu_nanophotonic_2023, @ray_optimized_2021, @zhang_sentiment_2023]. Among them, @ghosh_reservoir_2021, @liu_nanophotonic_2023 and @ray_optimized_2021 used it to forecast epidemic, @zhang_sentiment_2023 performed sentiment analysis and @kmet_bezier_2019 used it to solve optimal control related to vaccine. The evaluation of RC for epidemic forecast showed promising results in all approaches, being competitive with Long-Short Term Memory (LSTM) and Feed-Forward Neural Network (FFNN) in @ray_optimized_2021. However, the test period was short for @ghosh_reservoir_2021 (21 and 14 days) and @ray_optimized_2021 (86 days) making it difficult to evaluate the behavior of the methods during epidemic dynamic shift. This was not the case for @liu_nanophotonic_2023 (6 months) but they implemented daily ahead forecast which would be difficult to use to manage a hospital. Finally, all three implementations used only one time series as input whereas it has been shown that using different data sources could improve forecast [@ferte_benefit_2022]. Therefore, it is still difficult to assess the usefulness of RC over a large period and using many time series as inputs.

`TODO :` check for new published forecast models

RC can be approached as an extension of penalized linear regression where inputs are processed by a reservoir which allows memory and non-linear combinations. As penalized linear regression was the best approach for covid-19 forecast in @ferte_benefit_2022 and that RC showed promising results for epidemic forecast in @ghosh_reservoir_2021, @liu_nanophotonic_2023 and @ray_optimized_2021, we proposed to use RC to forecast hospitalizations at 14 days at the University Hospital of Bordeaux. The main objective of this study is to evaluate the performance of RC for this task. Secondary objective will be (i) to assess the performance of RC depending on the input dimension, (ii) to assess the relevance of Genetic Algorithm (GA) for hyperparameter optimization and feature selection in this context.

## Methods

### Data

The study utilized aggregated data spanning from May 16, 2020, to January 17, 2022, regarding the COVID-19 epidemic in France, drawing from various sources to enhance forecasting accuracy. These sources encompassed epidemiological statistics from Santé Publique France, weather data from the National Oceanic and Atmospheric Administration (NOAA), both providing department-level data [@etalab_les_2020, @smith_integrated_2011] and Electronic Health Record (EHR) data from the Bordeaux Hospital providing hospital-level data. All data were daily updated. Santé Publique France data included information on hospitalizations, RT-PCR tests, positive RT-PCR results, variant prevalence, and vaccination data, categorized by age groups. NOAA data contributed temperature, wind speed, humidity, and dew point data, allowing for the computation of the COVID-19 Climate Transmissibility Predict Index [@roumagnac_etude_2021]. EHRs data included hospitalizations, ICU admissions, ambulance service records, and emergency unit notes, with relevant COVID-19-related concepts extracted from the notes. Data are discussed more in depth in @ferte_benefit_2022.

First and second derivative over the last $7$ days were computed to enrich model information. To take into account measurement error and daily noise variation, data were smoothed using a local polynomial regression with a span of 21 days.

### Evaluation framework

The task was to forecast 14 days ahead the number of hospitalized patients. As seen at section [Regression](#basicregression), we will train the model to predict the variation of hospitalization defined as $outcome_{t+14} = hosp_{t+14} - hosp_{t}$. Metrics computation and visualizations will be performed on the predicted number of hospitalizations denoted as $\hat{y_{t+14}} = \widehat{outcome_{t+14}} + hospitalizations_{t}$ and the observed number of hospitalizations at time $t+14$ denoted as $y_{t+14} = hospitalizations_{t+14}$

The dataset was separated into two periods. First period from May 16, 2020 to March 1, 2021 served to identify relevant hyperparameters. Remaining data was used to evaluate the model performance. As the time series is not stationary and most appropriate hyperparameter value might change over time, we also investigated the effect of hyperparameter update every month.

The performance of the model was evaluated according to several metrics: the median absolute error ($MAE = median(|\hat{y_t}-y_t|)$) , the median relative error ($MRE = median(|\frac{\hat{y_t}-y_t}{y_t}|)$), the median absolute error to baseline ($MAEB =  median(|\hat{y_t}-y_t| - |y_{t-14} - y_t|)$), the median relative error to baseline ($MREB = median(\frac{|\hat{y_t}-y_t|}{|y_{t-14} - y_t|})$). Such that the model denoted as baseline was the model defined as $\hat{y_t} = y_{t-14}$. All those metrics reflects different information. $MAE$ and $MAEB$ are highly sensitive to forecast error when the number of hospitalizations is high. $MAEB$ encapsulates information about the current dynamic of the hospitalizations : for the same absolute error, absolute error to baseline will be lower if the number of hospitalization is not constant. **MAEB = redundant with MAE ?**. $MRE$ is highly sensitive to forecast error when the number of hospitalizations is low because the denominator will be small. $MREB$ is highly sensitive to small forecast error when the number of hospitalized patients is constant over time. For each metric, the median was chosen over the mean to be robust to outliers. For instance, for relative error, when the outcome is low (e.g equal to 1 hospitalization), an error of 5 hospitalizations could results to a 400% relative error. **But that might favor models with chaotic metric because extreme values won't be used to compute the error ? => keep median for relative error and mean of absolute error and optimize on MAE instead of MREB ? Also, because MAE is a difference it is similar if we compute it on the number of hospitalization or on the variation of the number of hospitalization. Also we might discuss to penalize more the error when the number of hospitalizations is high (already done implicitly by AE) and when there is a dynamic shift => ponderate AE by absolute value of second derivative ?**.

### Feature selection with RC

We investigated the performance of RC using various feature selection approach and input scaling configuration. The following setting were evaluated :

- Feature selection based on expert inputs with common input scaling
- Feature selection based on expert inputs with specific input scaling for each feature
- Genetic algorithm feature selection with common input scaling
- Genetic algorithm feature selection with specific input scaling for each feature
- Elastic-net feature selection with common input scaling

Experts inputs were a priori defined features that were judge important for epidemic forecast : hospitalizations, positive RT-PCR, positive RT-PCR among the 60 years old and more, fraction of positive RT-PCR, fraction of positive RT-PCR among the 60 years old and more, IPTCC, number of patient EHR mentioning "covid-19" in emergency units department, number of people having received at least one dose of vaccine in Gironde. Additionally we provided the first derivative at 7 days for each feature.

We used feature selection using elastic-net because it was the best model in previous experiment compared to other machine learning (@ferte_benefit_2022). This feature selection impose to select the penalty hyperparameter of elastic-net. This hyperparameter was selected using genetic algorithm as depicted [previously](#geneticalgo).

Genetic algorithm feature selection was performed using a binary hyperparameter for each feature determining if it was to be selected or not.

In addition, we investigated if monthly update of hyperparameter was improving the performance and if optimizing the random seed (which defines the connections inside the reservoir) was improving the results.

### Hyperparameter optimisation using random search {#geneticalgo}

RC relies mainly on 4 hyperparameters including the leaking rate (i.e "memory" parameter), spectral radius (i.e "chaoticity" parameter), input scaling (i.e "feature gain" parameter) and ridge (i.e penalization parameter). Input scaling can be either, common to all features or specific to each feature which increases the number of hyperparameter by the number of features. As the relevancy of feature for forecast evolves over time, we explored (i) filter based feature selection based on elastic-net before RC which has a penalization hyperparameter denoted as $enet$ and (ii) wrapper feature selection for which each feature has an additional binary hyperparameter defining if it should be selected or not.

Regarding the number of hyperparameter, an automatic approach is necessary. We chose to use a genetic algorithm (GA) which has the benefit of making few assumptions on the link function between the vector of hyperparameter value and the objective function but might need many trials which are possible due to the fast of RC (**TODO : add ref**). The objective function of GA was to minimize the $MREB$ metric **(change ?)**. The pseudo-code of GA is defined as follow :

For each hyperparameter, we must define the boundaries of the search space. We chose a large search space because we had few hypothesis to restrict it. Spectral radius and input scaling were defined on $[10^{-5} ; 10^5]$, leaking rate on $[10^{-5} ; 1]$, ridge on $[10^{-10} ; 10^5]$.

The penalization for elastic-net feature selection was initially defined on $[10^{-10} ; 10^5]$. However, first experiments showed that, after few generations, GA was stuck in a area were no feature selection was performed and without the ability to sample new individuals with feature selection. This phenomena is likely to append for evolutionary algorithm where the sampling variance is small compared to the search space. This phenomena is depicted at figure \@ref(fig:enetNbFeatures) were there is no feature selection performed when penalization is below $10^{-4}$. To mitigate this problem, we chose to restrict the search space on $[10^{-4} ; 10^5]$.

## Results

```{r}
ls_use_case <- readRDS(file = here::here("data/precomputed_results.rds"))
```


**TODO : change optimisation metric = mean absolute error ?**
**TODO : optimise the seed ?**
**TODO : compare with elastic-net alone ?**
**TODO : compare with LSTM ?**
**TODO : improve optimisation figures (too small)**
**TODO : do everything with the R API ?**
**TODO : provide clean replication code as a supplementary material (too long to be run inside a rmarkdown chunk)**

The goal of this task is to predict 14 days ahead the hospitalization curve depicted at figure \@ref(fig:casestudyhospitalizations). It shows both the training set (i.e before 2021-03-01) and the test set encompass several slope shifts which are difficult to forecast. We will first introduce the results of the genetic algorithm hyperparameter optimization and then present the forecast performance.

```{r ucPresentData, echo = FALSE, fig.cap = "Data of the use case. Outcome of interest is presented in orange. Model will be trained to forecast outcome variation but will be used to Outcome for performance and representation. Other features are represented in darkblue.", fig.height=10}
ls_use_case$plot_present_data
```

### Hyperparameter optimisation

**TODO : add final choice of hyperparameter value**

In this section we will first introduce the results for the features selected based on expert knowledge with multiple input scaling and for the features selected by genetic algorithm with multiple input scaling and seed optimization.

```{r ucHyperparam, fig.cap="Hyperparameter evaluation on training set by random search. Hp sets with MAE above 30 were removed for clarity of visualisation.", fig.height=12}
ls_use_case$plot_reservoir_hp / ls_use_case$plot_feature_input_scaling + plot_annotation(tag_levels = 'A')
```

### Number of model to aggregate

**TODO : if we keep this section, we should update with same hyperparameter (feature selection with only 30 features for GA multiple input scaling) and same aggregation mechanism as in the present forecast performance to have something clean.**
**Note : if we optimize the seed, then the repetition of a given set of hyperparameter makes no sense (i.e will provide same results each time).**

```{r ucAggregation, echo = FALSE, fig.cap = "Aggregation of several reservoir. Due to its intrisic chaoticity, a same hyperparameter set can results in heterogeneous forecast. Panel A shows the individual forecast of 40 repetition of the same hyperparameter set for the different RC configuration. Panel B shows the impact of aggregating several RC with the same set of HP. It has small interest when the input layer was not connected to output layer but significantly improves the performance otherwise.", fig.height=8}
ls_use_case$plot_before_aggregation_forecast + ls_use_case$plot_aggregation + plot_annotation(tag_levels = 'A')

```

### Present forecast performance

```{r perfMonthlyUpdate, echo = FALSE}
ls_use_case$table_perf_esn %>%
  knitr::kable(caption = "Model performance with several reservoir configuration. For each setting, 40 reservoir are computed and the forecast is the median of the 40 forecast. Results show the mean number of features and the performance metrics. MAE = Mean Absolute Error, MRE = Median Relative Error, MAEB = Mean Absolute Error to Baseline, MREB = Median Relative Error to Baseline.",
               col.names = c("Model", "MAE", "MRE", "MAEB", "MREB"),
               booktabs = TRUE,
               linesep = c('', '\\addlinespace'), digits = 2)
```

Table \@ref(tab:perfMonthlyUpdate) shows the performance on the test set. The number of selected features by elastic-net and by genetic algorithm were in the same order of magnitude. Updating the hyperparameter monthly did not always improve forecast accuracy (e.g epidemiological subset with multiple input scaling). Best model depend on the metric but epidemiological subset with multiple input scaling and no update was the best model or second to best model for all metrics.

```{r forecastwithorwithoutUpdate, echo = FALSE, fig.cap = "Reservoir computing forecast depending on the setting with and without monthly update. Red line is the median forecast of 40 reservoirs. Grey lines are individual forecast of each of the 40 reservoir.", fig.height=8}
ls_use_case$plot_forecast
```

Figure \@ref(fig:forecastwithorwithoutUpdate) shows the forecast with and without monthly hyperparameter update. Graphically we observe that the forecast of individual models (in grey) are chaotic and that forecast aggregation (in red) is necessary. This is less concerning for epidemiological subset with multiple input scaling or when the seed is controlled. In terms of performance, we observe similar conclusion as the one depicted for table \@ref(tab:perfMonthlyUpdate). However, the model errors did not occur for the same period for each model. For instance, the epidemiological with multiple scaling and monthly update error is located mainly during the winter 2022 increase. Model based on all features tend to overestimate the length of the summer 2021 wave.

## Discussion

# Discussion and conclusion

# Bibliography
