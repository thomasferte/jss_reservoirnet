---
documentclass: jss
author:
  - name: FirstName LastName
    orcid: 0000-0000-0000-0000
    affiliation: University/Company
    # use this syntax to add text on several lines
    address: |
      | First line
      | Second line
    email: \email{name@company.com}
    url: https://posit.co
  - name: Second Author
    orcid: 0000-0000-0000-0000
    affiliation: 'Affiliation \AND'
    # To add another line, use \AND at the end of the previous one as above
  - name: Third Author
    orcid: 0000-0000-0000-0000
    address: |
      | Department of Statistics and Mathematics,
      | Faculty of Biosciences,
      | Universitat Autònoma de Barcelona
    affiliation: |
      | Universitat Autònoma 
      | de Barcelona
    # use a different affiliation in adress field (differently formated here)
    affiliation2: Universitat Autònoma de Barcelona
title:
  formatted: "A Capitalized Title: Something about a Package \\pkg{foo}"
  # If you use tex in the formatted title, also supply version without
  plain:     "A Capitalized Title: Something about a Package foo"
  # For running headers, if needed
  short:     "\\pkg{foo}: A Capitalized Title"
abstract: >
  The abstract of the article.
keywords:
  # at least one keyword must be supplied
  formatted: [keywords, not capitalized, "\\proglang{Java}"]
  plain:     [keywords, not capitalized, Java]
preamble: >
  \usepackage{amsmath}
output:
  bookdown::pdf_book:
    base_format: rticles::jss_article
  bookdown::word_document2:
    default
knit: (function(inputFile, encoding){
  rmarkdown::render(inputFile, encoding = encoding,
  output_dir = here::here("output"), output_format = "all") })
---

```{r, setup, include=FALSE}
options(prompt = 'R> ', continue = '+ ')
```

# Introduction

## Reservoir networks presentation

Reservoir Computing (RC) is a prominent machine learning methodology
that has gained significant attention in recent years for its ability to
effectively process information generated by dynamical systems. This
innovative approach leverages the dynamics of a high-dimensional
reservoir to perform complex computations and solve various tasks based
on the response of this dynamical system to input signals. Reservoir
Computing is characterized by its unique architecture, which includes a
dynamic system, often referred to as a reservoir, that projects temporal
input signals onto a high-dimensional feature space. This
high-dimensional representation of input data enables Reservoir
Computing to excel in tasks such as time series prediction, pattern
recognition, and signal processing.

## Why reservoir are interesting compared to standard approaches (penalised regression) and deep learning approaches?

RC exhibits several advantages over its counterparts. It excels in tasks
that require the processing of temporal data, such as time series
forecasting and speech recognition, thanks to its inherent memory
capabilities. RC offers computational efficiency, reduced susceptibility
to overfitting, and a higher degree of interpretability compared to deep
learning models. Moreover, RC is able to capture intricate temporal
patterns compared to statistical methods such as penalized regression.

## examples of previous use

TODO: - Ghosh S, Senapati A, Mishra A, Chattopadhyay J, Dana SK, Hens C,
et al. Reservoir computing on epidemic spreading: A case study on
COVID-19 cases. Phys Rev E. 16 juill 2021;104(1):014308. - Existing
review?

## challenges of using reservoir

RC use is stille challenging for several reasons. First, its stochastic
nature due to the reservoir random part makes the results of a given RC
unpredictable even if the hyperparameters are well chosen. Second,
eventhough, RC relies on several hyperparameter to determine the
connections between neurons. There exist few guidance about the way to
set the hyperparameters and most often the user must rely on a wrapper
approach were combinations of hyperparameters performance are evaluated
on a train set before chosing the ideal combination one. Third, most
published paper are based on a low dimensional setting and few guidance
exist in the context of high dimensional data, especially considering
the choice of hyperparameters. Fourth, the use of reservoir computing in
an epidemic setting with highly non-stationary time serie is scarce
whereas it is of tremendous interest for epidemiologists. Fifth, there
is currently no implementation available on R making the use of this
method challenging for user not familiar with Python.

## what we do

In this paper we propose to explore those challenges with extensive
guidance in order to help new user to fully benefit from RC. First a
general introduction to reservoir computing is provided with a tutorial
for its use using reservoirnet, a R package based on reservoirPy Python
module. Second, we explore the different challenges of the use of RC in
a non-stationary high dimensional setting based on the use case of
covid-19 hospitalisations forecast with extensive guidance on the
modelling strategy, the choice of hyperparameters using a genetic
algorithm and the implementation.

# RC presentation

RC is a machine learning algorithm composed of an input layer denoted
$u(t)$ which is randomy transformed by the reservoir into an activation
state denoted $x(t)$ which is fed to a ridge penalized linear regression
trained to foreast the outcome denoted as $y(t)$ as depicted at figure
\@ref(fig:rcpresentation).

```{r rcpresentation, echo=FALSE, fig.cap="my caption", out.width = '90%', fig.cap="Reservoir computing is composed of an input layer, a reservoir and an output layer. Connection between input layer and reservoir and inside reservoir are random. Only the output layer is optimized based on a ridge penalized linear regression."}
knitr::include_graphics(here::here("reservoirnet : an R package for reservoir computing/images/schema.png/image1.png"))
```

The input layer $u(t)$ is an $M$-dimension vector which corresponds to
the values of the input time series at time $t$ where $t = 1, …, T$. The
reservoir layer $x(t)$ is an $N_{res}$-dimensional vector where
$N_{res}$ is the number of nodes in the reservoir. The value $x(t)$ is
defined as follow:

$$x( t+1 ) = ( 1 - \alpha )  x ( t) + \alpha \: tanh( W x(t) + W_{in} u(t+1) ) \text{ where } \alpha \in [0, 1 ]$$

The leaking rate alpha define the rate of update of the nodes. The
closer $\alpha$ is to $1$, the higher the reservoir is sensitive to new
inputs (i.e $u(t)$). Therefore, the reservoir state at time $t+1$
denoted $x(t+1)$ depends on the reservoir state at the previous time
(i.e $x(t)$) and the new inputs (i.e $u(t+1)$). Both $W_{in}$ and $W$
are random matrices of size $Nres \times M$ and $Nres \times Nres$
respectively.

$W_{in}$ is a dense matrix generated using a bernouilli distribution
where each value can be either $-I_{scale}(m)$ or $I_{scale}(m)$ with an
equal probability where $m = 1, …, M$ corresponds to a given feature in
the input layer. The input scaling, denoted $I_{scale}$, is an
hyperparameter coefficient which can be common to all features from the
input layer or specific to each feature $m$. In that case, the more
important the feature is, the greater should be its input scaling. $W$
is a sparse matrix where values are generated base on a gaussian
distribution $\mathcal{N}(0,1)$. Then the matrix $W$ is rescaled
according to the spectral radius, an hyperparameter defining the highest
eigen value of $W$.

The final layer is a linear regression with ridge penalization where the
explanatory features are the reservoir state and the variable to be
explain is the outcome to predict such that:

$$W_{out} = YX^T ( XX^T + \lambda  I)^{ -1 }$$

Where x(t) and y(t) are accumulated in X and Y respectively such that:

$$X = \begin{bmatrix} x(1) \\ x(2) \\ ... \\ x(T) \end{bmatrix}
\text{ and } Y = \begin{bmatrix} y(1) \\ y(2) \\ ... \\ y(T) \end{bmatrix}$$

The parameter $\lambda$ is the ridge penalisation which aims to prevent
overfitting.

Overall, there are four main hyperparameters to be chosen by the user:
the leaking rate wich defines the memory of the RC, the input scaling
wich define the relative importance of the features, the spectral radius
wich define the connections of the neurons inside the reservoir and the
ridge penalization wich defines the degree of overfitting. The choice of
hyperparameter is difficult and the use of a wrapper approach where the
performance of the RC with different combinations of hyperparameters is
evaluated on a train set, the best combination is chosen for the use on
the test set.

RC in high dimension

The interest of RC in high dimensional setting is unclear for at least
two reasons: (i) RC relies primarily on the projection of the low
dimension input layer to the high dimension space of the reservoir
neurons and (ii) the high number of features for which each can be
associated with a different input scaling increases the hyperparameter
search space making standard approaches such as random search or grid
search inefficient.

To tackle both problems, we proposed a genetic algorithm for
hyperparameter optimisation including feature selection.

1.  Initialize :

-   Population parameters : $N_{pop} = 200$, $N_e = 100$,
    $N_{generation} = 30$
-   Cross-over parameters : $N_{tournament} = 2$
-   Mutation parameters : $p_{mutQuant} = .5$, $p_{mutCat} = .25$,
    $\sigma = 1$ ($\sigma = 0.1$ for leaking rate as the hyperparameter
    range is shorter)
-   Define the $hyperparameter\_list$, a list containing each
    hyperparameter denoted as $param$.
-   `for` each $param$ in $hyperparameter\_list$ define hyperparameter
    $search\_space$ :
    -   the $lower$ and $upper$ bounds,
    -   the $type$ (numeric, integer, categorical)
    -   if it should be $log$ transformed (True, False)

2.  Define main functions
    -   Define $tournamentSelection(N_{pop}, N_{tournament})$:
        -   Get the best $N_{pop}$ individuals from previous generations
        -   Randomly select $N_{tournament}$ challengers among them
        -   Get the best individuals from the challengers
        -   Repeat twice to get a father and a mother
        -   `return` $father, \: mother$
    -   Define
        $crossoverMutation(pere\$param\$value, mere\$param\$value, param\$search\_space)$:
        -   `if` $search\_space\$log$ $\newline$
            $\text{} \qquad pere\$param\$value = log_{10}(pere\$param\$value)$
            $\newline$
            $\text{} \qquad mere\$param\$value = log_{10}(mere\$param\$value)$

        -   sample $alp$ and $mut$ probabilities from $\mathcal{U}(0,1)$

        -   Cross-over, and define $child\$param\$value$ as :

            -   Numerical :
                $alp*pere\$param\$value + (1-alp)*mere\$param\$value$
            -   Integer :
                $round( alp*pere\$param\$value + (1-alp)*mere\$param\$value)$
            -   Categorical :
                $sample(x = c(pere\$param\$value, mere\$param\$value), \newline\text{}\qquad\text{}\qquad\text{}\qquad\text{}\qquad\text{}\qquad prob = c(alp, 1-alp))$

        -   `if` $mut < p_{mutQuant}$ or $mut < p_{mutCat}$ for
            quantitative and categorical features respectively, mutate :

            -   Categorical : sample another modality
            -   Integer : add or substract 1 at random
            -   Numerical : Add $\sigma \times \mathcal{N}(0,1)$

        -   `if` $search\_space\$log$ :
            $child\$param\$value = 10^{child\$param\$value}$

        -   `return` $child\$param\$value$
    -   Define $geneticSearch(N_{pop}, N_{tournament})$ :
        -   $pere, \: mere = tournamentSelection(N_{pop}, N_{tournament})$
        -   `for` $param$ in $hyperparameter\_list$ : $\newline$
            $\text{} \qquad child\$param\$value = crossoverMutation(\newline\text{}\qquad\text{}\qquad\text{}\qquad\text{}\qquad\text{}\qquad pere\$param\$value, \newline\text{}\qquad\text{}\qquad\text{}\qquad\text{}\qquad\text{}\qquad mere\$param\$value, \newline\text{}\qquad\text{}\qquad\text{}\qquad\text{}\qquad\text{}\qquad param\$search\_space)$
        -   `return` $child$
3.  Run the algorithm
    -   Step 1 : Initialize genetic algorithm
        -   Sample $N_{pop}$ individuals (i.e set of hyperparameters)
            using random search
    -   Step 2 : Run genetic algorithm
        -   $generation = 1$
        -   `while` $generation < Ngeneration$ :
            -   $generation = generation + 1$
            -   sample $N_{pop}$ individuals using $geneticSearch$
                function

# Basic package use

In this section, we will cover the basics of reservoirnet use including
installation, classification and regression. We will provide brief
overview of the package use and more in depth description is provided in
section 4 with the covid-19 forecast use case.

## Installation

reservoirnet is an R package api making the python module reservoirPy
easily callable from R. It is available on CRAN (see
<https://cran.r-project.org/package=reservoirnet>) and can be installed
using:

```{r eval=FALSE}
install.packages("reservoirnet")
```

## Classification

Reservoir Computing (RC) is well suited to both regression and classification tasks. We will introduce a simple example of classification task. This chapter is largely inspired from reservoirnet vignette on
[classification](https://cran.r-project.org/web/packages/reservoirnet/vignettes/Classification_with_RC.html).

The first step is to load useful packages. For this example we will load both reservoirnet and ggplot2 (add ref) a useful package for datavisualisation.

```{r}
library(reservoirnet)
library(ggplot2)
```

### The Japanese vowel dataset

The Japanese vowel dataset is composed of 640 utterances of the Japanese vowel \ae, from 9 different male speakers. The goal of this task is to assign to each utterance the label of its speaker. Dataset is split between a 270 utterances training set and a 340 utterances testing set. Each spoken utterance is a timeseries of 7\~29 timesteps. Each timestep of signal is a 12 dimensional vector representing Linear Prediction Coefficient (LPC), which encode the audio signal into the cepstral domain (a variant of the frequency domain).

References M. Kudo, J. Toyama and M. Shimbo. (1999). "Multidimensional
Curve Classification Using Passing-Through Regions". Pattern Recognition
Letters, Vol. 20, No. 11--13, pages 1103--1111.
<https://archive.ics.uci.edu/ml/machine-learning-databases/JapaneseVowels-mld/>

The data set is splitted into a train and a test set.

```{r}
japanese_vowels <- reservoirnet::generate_data(dataset = "japanese_vowels")$japanese_vowels

X_train <- japanese_vowels$X_train
Y_train <- japanese_vowels$Y_train
X_test <- japanese_vowels$X_test
Y_test <- japanese_vowels$Y_test
```

```{r fig.cap="Vowel dataset"}
sample_per_speaker <- 30
n_speaker <- 9
X_train_per_speaker <- list()

for (i in 0:8) {
 X_speaker <- X_train[((i*sample_per_speaker)+1):((i+1)*sample_per_speaker)]
 X_train_per_speaker[[i+1]]<-(as.numeric(unlist(sapply(X_speaker, t))))
}

d <- data.frame(LPC = unlist(X_train_per_speaker), 
                Speaker = factor(rep(1:9,times = sapply(X_train_per_speaker,length))))

# plot data
ggplot(d,aes(x = Speaker, y = LPC)) + geom_boxplot()+theme_bw()
```

### Echo state network setup

#### Transduction (sequence-to-sequence model)

As ReservoirPy Nodes are built to work on sequences, the simplest setup
to solve this task is sequence-to-sequence encoding, also called
transduction. A model is trained on encoding each vector of input
sequence into a new vector in the output space. Thus, a sequence of
audio yields a sequence of label, one label per timestep.

```{r}
japanese_vowels <- reservoirnet::generate_data(
    dataset = "japanese_vowels",
    repeat_targets=TRUE)$japanese_vowels

X_train <- japanese_vowels$X_train
Y_train <- japanese_vowels$Y_train
X_test <- japanese_vowels$X_test
Y_test <- japanese_vowels$Y_test
```

#### Train a simple Echo State Network to solve this task:

```{r}
source <- createNode("Input")
readout <- createNode("Ridge",ridge=1e-6)
reservoir <- createNode("Reservoir",units = 500,lr=0.1, sr=0.9)

#[source >> reservoir, source] >> readout
model <- list(source %>>% reservoir, source) %>>% readout
```

#### Fit the model

```{r}
model_fit <- reservoirnet::reservoirR_fit(node = model,
                                       X = X_train,
                                       Y = Y_train,
                                       stateful = FALSE,
                                       warmup = 2)

Y_pred <- reservoirnet::predict_seq(node = model_fit$fit,
                                 X = X_test,
                                 stateful = FALSE)
```

Get the scores:

There are 9 speakers, hence the output space is 9-dimensional. The
speaker label is the index of the output neuron with maximum activation.

```{r}
accuracy <- function(pred, truth){
  mean(pred == truth)
}
```

```{r}
Y_pred_class <- sapply(Y_pred, FUN = function(x) apply(as.matrix(x),1,which.max))
Y_test_class <- sapply(Y_test, FUN = function(x) apply(as.matrix(x),1,which.max))
score <- accuracy(array(unlist(Y_pred_class)), array(unlist(Y_test_class)))

print(paste0("Accuracy: ", round(score * 100,3) ,"%"))
```

    ## [1] "Accuracy: 91.296%"

### Classification (sequence-to-vector model)

We can create a more elaborated model where inference is performed only
once on the whole input sequence. Indeed, we only need to assign one
label to each input sequence. This new setup is known as a
sequence-to-vector model, and this is usually the type of model we refer
to when talking about classification of sequencial patterns.

```{r}
japanese_vowels <- reservoirnet::generate_data(
    dataset = "japanese_vowels")$japanese_vowels

X_train <- japanese_vowels$X_train
Y_train <- japanese_vowels$Y_train
X_test <- japanese_vowels$X_test
Y_test <- japanese_vowels$Y_test
```

```{r}
source <- reservoirnet::createNode("Input")
readout <- reservoirnet::createNode("Ridge",ridge=1e-6)
reservoir <- reservoirnet::createNode("Reservoir", units = 500, lr=0.1, sr=0.9)

#source >> reservoir >> readout
model <- source %>>% reservoir %>>% readout
```

We need to modify the training loop by hand a bit to perform this task:

first, we compute all reservoir states over the input sequence using the
reservoir.run method. then, we gather in a list only the last vector of
the states sequence.

```{r}
states_train = list()
k <- 1
for (x in X_train) {
  states <- reservoirnet::predict_seq(node = reservoir, X = x, reset=TRUE)
  states_train[[k]] <- t(as.matrix(states[nrow(states),]))
  k <- k+1
}
```

We can now train the readout only on the last state vectors. Here,
Y_train is an array storing a single label for each utterance.

```{r}
res <- reservoirnet::reservoirR_fit(readout,X = states_train, Y = Y_train)
summary(res)
```

    ## Parametrs using to fit:
    ##  warmup: 0 ; stateful: FALSE ; reset: FALSE 
    ## results of fitting:
    ## 'Ridge-5': Ridge(ridge=1e-06, input_bias=True, in=500, out=9)

We also modify the inference code using the same method as above:

```{r}
Y_pred <- list()
k <- 1
for (x in X_test) {
  states <- reservoirnet::predict_seq(node = reservoir, X = x, reset=TRUE)
  y <- reservoirnet::predict_seq(node = readout, X = as.array(states[nrow(states),]))
  Y_pred[[k]] <- y
  k <- k+1
}
```

```{r}
Y_pred_class <- sapply(Y_pred, FUN = function(x) apply(as.matrix(x),1,which.max))
Y_test_class <- sapply(Y_test, FUN = function(x) apply(as.matrix(x),1,which.max))

score <- accuracy(pred = Y_test_class,
                       truth = Y_pred_class)

print(paste0("Accuracy: ", round(score * 100,3) ,"%"))
```

    ## [1] "Accuracy: 88.108%"

## Regression

Vignette:
<https://cran.r-project.org/web/packages/reservoirnet/vignettes/basic_usage_01.html>

# Study case: Prediction covid-19

what is covid-19 and covid-19 pandemic

hospital is a central actor during pandemic, importance to anticipate
high number of patients

several models tried to forecast hospitalisations but task is difficult
and many models struggled to perform well.

In this setting, reservoir computing seem to be an appropriate approach
regarding the task.

4.1) Study case

This use case is based on data from Bordeaux University Hospital. During
the pandemic a predictive model was elaborate and regularly use to
forecast 14 days ahead the number of hospitalized patients at the
hospital. In this work we will compare the first approach based on
linear regression with elastic-net penalisation to reservoir computing
approach.

4.2) Evaluation framework

-   Train and test set

The task was to forecast 14 days ahead the number of hospitalised
patients. The dataset was separated into two periods. First period from
xx to xx served to identify relevant hyperparameters. Second period
after xx was used to evaluate the model performance. As the time serie
is not stationary and most appropriate hyperparameter value might change
over time, we also investigated the effect of hyperparameter update
every month.

The performance of the model was evaluated according to several metrics:
the median absolute error, the median relative error, the absolute error
relative to baseline, the relative error relative to baseline. For each
metric, the median was chosen over the mean to be robust to outliers.
For instance, for relative error, when the outcome is low (e.g equal to
1 hospitalisation), an error of 5 hospitalisations could results to a
500% relative error.

Median absolute error (AE): Median relative error (RE):

-   Baseline model (forecast at T+14 = observed at T)
-   Metrics

4.3) Hyperparameter optimisation using GA

-   Recall hyperparameters
-   Present GA algorithm + explain why it is important here (large
    number of hp + assumption free compared to other algorithm, cons =
    expensive)

4.4) Present results from GA

-   Present results and detail them

5)  Discussion and conclusion

<!-- ## Code formatting -->

<!-- In general, don't use Markdown, but use the more precise LaTeX commands instead: -->

<!-- * \proglang{Java} -->

<!-- * \pkg{plyr} -->

<!-- One exception is inline code, which can be written inside a pair of backticks (i.e., using the Markdown syntax). -->

<!-- If you want to use LaTeX commands in headers, you need to provide a `short-title` attribute. You can also provide a custom identifier if necessary. See the header of Section \ref{r-code} for example. -->

<!-- # \proglang{R} code {short-title="R code" #r-code} -->

<!-- Can be inserted in regular R markdown blocks. -->

<!-- ```{r} -->

<!-- x <- 1:10 -->

<!-- x -->

<!-- ``` -->

<!-- ## Features specific to \pkg{rticles} {short-title="Features specific to rticles"} -->

<!-- * Adding short titles to section headers is a feature specific to \pkg{rticles} (implemented via a Pandoc Lua filter). This feature is currently not supported by Pandoc and we will update this template if [it is officially supported in the future](https://github.com/jgm/pandoc/issues/4409). -->

<!-- * Using the `\AND` syntax in the `author` field to add authors on a new line. This is a specific to the `rticles::jss_article` format. -->
