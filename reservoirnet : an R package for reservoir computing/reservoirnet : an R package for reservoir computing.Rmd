---
documentclass: jss
author:
  - name: FirstName LastName
    orcid: 0000-0000-0000-0000
    affiliation: University/Company
    # use this syntax to add text on several lines
    address: |
      | First line
      | Second line
    email: \email{name@company.com}
    url: https://posit.co
  - name: Second Author
    orcid: 0000-0000-0000-0000
    affiliation: 'Affiliation \AND'
    # To add another line, use \AND at the end of the previous one as above
  - name: Third Author
    orcid: 0000-0000-0000-0000
    address: |
      | Department of Statistics and Mathematics,
      | Faculty of Biosciences,
      | Universitat Autònoma de Barcelona
    affiliation: |
      | Universitat Autònoma 
      | de Barcelona
    # use a different affiliation in adress field (differently formated here)
    affiliation2: Universitat Autònoma de Barcelona
title:
  formatted: "A Capitalized Title: Something about a Package \\pkg{foo}"
  # If you use tex in the formatted title, also supply version without
  plain:     "A Capitalized Title: Something about a Package foo"
  # For running headers, if needed
  short:     "\\pkg{foo}: A Capitalized Title"
abstract: >
  The abstract of the article.
keywords:
  # at least one keyword must be supplied
  formatted: [keywords, not capitalized, "\\proglang{Java}"]
  plain:     [keywords, not capitalized, Java]
preamble: >
  \usepackage{amsmath}
output:
  bookdown::pdf_book:
    base_format: rticles::jss_article
---

```{r, setup, include=FALSE}
options(prompt = 'R> ', continue = '+ ')
```

# Introduction

## Reservoir networks presentation

Reservoir Computing (RC) is a prominent machine learning methodology that has gained significant attention in recent years for its ability to effectively process information generated by dynamical systems. This innovative approach leverages the dynamics of a high-dimensional reservoir to perform complex computations and solve various tasks based on the response of this dynamical system to input signals.
Reservoir Computing is characterized by its unique architecture, which includes a dynamic system, often referred to as a reservoir, that projects temporal input signals onto a high-dimensional feature space. This high-dimensional representation of input data enables Reservoir Computing to excel in tasks such as time series prediction, pattern recognition, and signal processing.

## Why reservoir are interesting compared to standard approaches (penalised regression) and deep learning approaches?

RC exhibits several advantages over its counterparts. It excels in tasks that require the processing of temporal data, such as time series forecasting and speech recognition, thanks to its inherent memory capabilities. RC offers computational efficiency, reduced susceptibility to overfitting, and a higher degree of interpretability compared to deep learning models. Moreover, RC is able to capture intricate temporal patterns compared to statistical methods such as penalized regression.

## examples of previous use

TODO:
- Ghosh S, Senapati A, Mishra A, Chattopadhyay J, Dana SK, Hens C, et al. Reservoir computing on epidemic spreading: A case study on COVID-19 cases. Phys Rev E. 16 juill 2021;104(1):014308.
- Existing review?


## challenges of using reservoir

RC use is stille challenging for several reasons. First, its stochastic nature due to the reservoir random part makes the results of a given RC unpredictable even if the hyperparameters are well chosen. Second, eventhough, RC relies on several hyperparameter to determine the connections between neurons. There exist few guidance about the way to set the hyperparameters and most often the user must rely on a wrapper approach were combinations of hyperparameters performance are evaluated on a train set before chosing the ideal combination one. Third, most published paper are based on a low dimensional setting and few guidance exist in the context of high dimensional data, especially considering the choice of hyperparameters. Fourth, the use of reservoir computing in an epidemic setting with highly non-stationary time serie is scarce whereas it is of tremendous interest for epidemiologists. Fifth, there is currently no implementation available on R making the use of this method challenging for user not familiar with Python.

## what we do

In this paper we propose to explore those challenges with extensive guidance in order to help new user to fully benefit from RC. First a general introduction to reservoir computing is provided with a tutorial for its use using reservoirnet, a R package based on reservoirPy Python module. Second, we explore the different challenges of the use of RC in a non-stationary high dimensional setting based on the use case of covid-19 hospitalisations forecast with extensive guidance on the modelling strategy, the choice of hyperparameters using a genetic algorithm and the implementation.

# RC presentation

RC is a machine learning algorithm composed of an input layer denoted $u(t)$ which is randomy transformed by the reservoir into an activation state denoted $x(t)$ which is fed to a ridge penalized linear regression trained to foreast the outcome denoted as $y(t)$ as depicted at figure \@ref(fig:rcpresentation).

```{r rcpresentation, echo=FALSE, fig.cap="my caption", out.width = '90%', fig.cap="Reservoir computing is composed of an input layer, a reservoir and an output layer. Connection between input layer and reservoir and inside reservoir are random. Only the output layer is optimized based on a ridge penalized linear regression."}
knitr::include_graphics(here::here("reservoirnet : an R package for reservoir computing/images/schema.png/image1.png"))
```

The input layer $u(t)$ is an $M$-dimension vector which corresponds to the values of the input time series at time $t$ where $t = 1, …, T$. The reservoir layer $x(t)$ is an $N_{res}$-dimensional vector where $N_{res}$ is the number of nodes in the reservoir. The value $x(t)$ is defined as follow:

$$x( t+1 ) = ( 1 - \alpha )  x ( t) + \alpha \: tanh( W x(t) + W_{in} u(t+1) ) \text{ where } \alpha \in [0, 1 ]$$

The leaking rate alpha define the rate of update of the nodes. The closer $\alpha$ is to $1$, the higher the reservoir is sensitive to new inputs (i.e $u(t)$). Therefore, the reservoir state at time $t+1$ denoted $x(t+1)$ depends on the reservoir state at the previous time (i.e $x(t)$) and the new inputs (i.e $u(t+1)$). Both $W_{in}$ and $W$ are random matrices of size $Nres \times M$ and $Nres \times Nres$ respectively.

$W_{in}$ is a dense matrix generated using a bernouilli distribution where each value can be either $-I_{scale}(m)$ or $I_{scale}(m)$ with an equal probability where $m = 1, …, M$ corresponds to a given feature in the input layer. The input scaling, denoted $I_{scale}$, is an hyperparameter coefficient which can be common to all features from the input layer or specific to each feature $m$. In that case, the more important the feature is, the greater should be its input scaling. $W$ is a sparse matrix where values are generated base on a gaussian distribution $\mathcal{N}(0,1)$. Then the matrix $W$ is rescaled according to the spectral radius, an hyperparameter defining the highest eigen value of $W$.

The final layer is a linear regression with ridge penalization where the explanatory features are the reservoir state and the variable to be explain is the outcome to predict such that:

$$W_{out} = YX^T ( XX^T + \lambda  I)^{ -1 }$$

Where x(t) and y(t) are accumulated in X and Y respectively such that:

$$X = \begin{bmatrix} x(1) \\ x(2) \\ ... \\ x(T) \end{bmatrix}
\text{ and } Y = \begin{bmatrix} y(1) \\ y(2) \\ ... \\ y(T) \end{bmatrix}$$

The parameter $\lambda$ is the ridge penalisation which aims to prevent overfitting.

Overall, there are four main hyperparameters to be chosen by the user: the leaking rate wich defines the memory of the RC, the input scaling wich define the relative importance of the features, the spectral radius wich define the connections of the neurons inside the reservoir and the ridge penalization wich defines the degree of overfitting. The choice of hyperparameter is difficult and the use of a wrapper approach where the performance of the RC with different combinations of hyperparameters is evaluated on a train set, the best combination is chosen for the use on the test set.

RC in high dimension

The interest of RC in high dimensional setting is unclear for at least two reasons: (i) RC relies primarily on the projection of the low dimension input layer to the high dimension space of the reservoir neurons and (ii) the high number of features for which each can be associated with a different input scaling increases the hyperparameter search space making standard approaches such as random search or grid search inefficient.

To tackle both problems, we proposed a genetic algorithm for hyperparameter optimisation including feature selection.

1. Initialize :
  - Population parameters : Npop = 200, Ne = 100, Ngeneration = 30
  - Cross-over parameters : Ntournament = 2 
  - Mutation parameters : pmutQuant = .5, pmutCat = .25, sigma = 1 (0.1 for leaking rate as the hyperparameter range is shorter)
  - Define the hyperparameter_list, a list containing each param.
  - For each param in hyperparameter_list define hyperparameter search_space : define the lower and upper bound, the type (numeric, integer, categorical) and if it should be log transformed (True, False)

2. Define tournamentSelection(Npop, Ntournament): 
    - Get the best Npop individuals from previous generations 
    - Randomly select Ntournament challengers among them
    - Get the best individuals from the challengers
    - Repeat twice to get a father and a mother 
    - return father, mother 
3. Define crossoverMutation(pere.param.value, mere.param.value, param.search_space): 
    - sample alp and mut from a Uniform(0,1) distribution
    - If search_space.log is True then  pere.param.value, mere.param.value = log10(pere.param.value), log10(mere.param.value)
    - Cross-over, and define child.param.value as :
        - Numerical : alp*pere.param.value + (1-alp)*mere.param.value
        - Integer : round( alp*pere.param.value + (1-alp)*mere.param.value)
        - Categorical : random choice (list = [pere.param.value, mere.param.value], prob = [alp, 1-alp]) 
    - Mutation if mut < pmutQuant or mut < pmutCat for quantitative and categorical features respectively and update child.param.value
        - Categorical : Randomly selected an other modality 
        - Integer : Randomly add or substract 1 
        - Numerical : Add sigma * N(0,1) 
    - If search_space.log is True then child.param.value = 10^child.param.value
    - Return child.param.value

4. Define geneticSearch(Npop, Ntournament) :
    - pere, mere = tournamentSelection(Npop, Ntournament)
    - for param in  hyperparameter_list :
        - child.param.value = crossoverMutation(pere.param.value, mere.param.value, param.search_space)
    - return child.param

Step 1 : Initialize genetic algorithm

- Sample Npop individuals (i.e set of hyperparameters) using random search

Step 2 : Run genetic algorithm
- generation = 1
While generation <  Ngeneration :	
    generation = generation + 1
    sample Npop individuals using geneticSearch method

3) Basic package use

In this section, we will cover the basics of reservoirnet use including installation, classification and regression. We will provide brief overview of the package use and more in depth description is provided in section 4 with the covid-19 forecast use case.

3.1) Installation

reservoirnet is an R package api making the python module reservoirPy easily callable from R. It is available on CRAN (see https://cran.r-project.org/package=reservoirnet) and can be installed using:

install.packages("reservoirnet")

3.2) Classification

Vignette:
https://cran.r-project.org/web/packages/reservoirnet/vignettes/Classification_with_RC.html 

3.3) Regression

Vignette:
https://cran.r-project.org/web/packages/reservoirnet/vignettes/basic_usage_01.html 

4) Study case: Prediction covid-19

what is covid-19 and covid-19 pandemic

hospital is a central actor during pandemic, importance to anticipate high number of patients

several models tried to forecast hospitalisations but task is difficult and many models struggled to perform well.

In this setting, reservoir computing seem to be an appropriate approach regarding the task.

4.1) Study case

This use case is based on data from Bordeaux University Hospital. During the pandemic a predictive model was elaborate and regularly use to forecast 14 days ahead the number of hospitalized patients at the hospital. In this work we will compare the first approach based on linear regression with elastic-net penalisation to reservoir computing approach.

4.2) Evaluation framework

- Train and test set

The task was to forecast 14 days ahead the number of hospitalised patients. The dataset was separated into two periods. First period from xx to xx served to identify relevant hyperparameters. Second period after xx was used to evaluate the model performance. As the time serie is not stationary and most appropriate hyperparameter value might change over time, we also investigated the effect of hyperparameter update every month.

The performance of the model was evaluated according to several metrics: the median absolute error, the median relative error, the absolute error relative to baseline, the relative error relative to baseline. For each metric, the median was chosen over the mean to be robust to outliers. For instance, for relative error, when the outcome is low (e.g equal to 1 hospitalisation), an error of 5 hospitalisations could results to a 500% relative error.

Median absolute error (AE): 
Median relative error (RE): 



- Baseline model (forecast at T+14 = observed at T)
- Metrics



4.3) Hyperparameter optimisation using GA

- Recall hyperparameters
- Present GA algorithm + explain why it is important here (large number of hp + assumption free compared to other algorithm, cons = expensive)

4.4) Present results from GA

- Present results and detail them


5) Discussion and conclusion

<!-- ## Code formatting -->

<!-- In general, don't use Markdown, but use the more precise LaTeX commands instead: -->

<!-- * \proglang{Java} -->
<!-- * \pkg{plyr} -->

<!-- One exception is inline code, which can be written inside a pair of backticks (i.e., using the Markdown syntax). -->

<!-- If you want to use LaTeX commands in headers, you need to provide a `short-title` attribute. You can also provide a custom identifier if necessary. See the header of Section \ref{r-code} for example. -->

<!-- # \proglang{R} code {short-title="R code" #r-code} -->

<!-- Can be inserted in regular R markdown blocks. -->

<!-- ```{r} -->
<!-- x <- 1:10 -->
<!-- x -->
<!-- ``` -->

<!-- ## Features specific to \pkg{rticles} {short-title="Features specific to rticles"} -->

<!-- * Adding short titles to section headers is a feature specific to \pkg{rticles} (implemented via a Pandoc Lua filter). This feature is currently not supported by Pandoc and we will update this template if [it is officially supported in the future](https://github.com/jgm/pandoc/issues/4409). -->
<!-- * Using the `\AND` syntax in the `author` field to add authors on a new line. This is a specific to the `rticles::jss_article` format. -->
